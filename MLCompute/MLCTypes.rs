//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
use objc2::__framework_prelude::*;
use objc2_foundation::*;

use crate::*;

/// A callback completion handler you execute when a graph finishes execution.
///
/// Parameters:
/// - resultTensor: The result tensor.
///
/// - error: An error if one occured, otherwise `nil`.
///
/// - executionTime: The execution time.
///
/// A callback completion handler you execute when a graph finishes execution.
#[cfg(all(feature = "MLCTensor", feature = "block2"))]
pub type MLCGraphCompletionHandler =
    *mut block2::DynBlock<dyn Fn(*mut MLCTensor, *mut NSError, NSTimeInterval)>;

/// A tensor data type.
/// A tensor data type.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCDataType(pub i32);
impl MLCDataType {
    #[doc(alias = "MLCDataTypeInvalid")]
    pub const Invalid: Self = Self(0);
    /// The 32-bit floating-point data type.
    #[doc(alias = "MLCDataTypeFloat32")]
    pub const Float32: Self = Self(1);
    /// The 16-bit floating-point data type.
    #[doc(alias = "MLCDataTypeFloat16")]
    pub const Float16: Self = Self(3);
    /// The Boolean data type.
    #[doc(alias = "MLCDataTypeBoolean")]
    pub const Boolean: Self = Self(4);
    /// The 64-bit integer data type.
    #[doc(alias = "MLCDataTypeInt64")]
    pub const Int64: Self = Self(5);
    /// The 32-bit integer data type.
    #[doc(alias = "MLCDataTypeInt32")]
    pub const Int32: Self = Self(7);
    /// The 8-bit integer data type.
    #[doc(alias = "MLCDataTypeInt8")]
    pub const Int8: Self = Self(8);
    /// The 8-bit unsigned integer data type.
    #[doc(alias = "MLCDataTypeUInt8")]
    pub const UInt8: Self = Self(9);
    /// The 8-bit unsigned integer data type.
    #[doc(alias = "MLCDataTypeCount")]
    pub const Count: Self = Self(10);
}

unsafe impl Encode for MLCDataType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCDataType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// An initializer type you use to create a tensor with random data.
/// An initializer type you use to create a tensor with random data.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCRandomInitializerType(pub i32);
impl MLCRandomInitializerType {
    #[doc(alias = "MLCRandomInitializerTypeInvalid")]
    pub const Invalid: Self = Self(0);
    /// The uniform random initializer type.
    #[doc(alias = "MLCRandomInitializerTypeUniform")]
    pub const Uniform: Self = Self(1);
    /// The glorot uniform random initializer type.
    #[doc(alias = "MLCRandomInitializerTypeGlorotUniform")]
    pub const GlorotUniform: Self = Self(2);
    /// The Xavier random initializer type.
    #[doc(alias = "MLCRandomInitializerTypeXavier")]
    pub const Xavier: Self = Self(3);
    /// The Xavier random initializer type.
    #[doc(alias = "MLCRandomInitializerTypeCount")]
    pub const Count: Self = Self(4);
}

unsafe impl Encode for MLCRandomInitializerType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCRandomInitializerType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A device type for execution of a neural network.
/// A device type for execution of a neural network.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCDeviceType(pub i32);
impl MLCDeviceType {
    /// A device type that represents the CPU.
    /// The CPU device
    #[doc(alias = "MLCDeviceTypeCPU")]
    pub const CPU: Self = Self(0);
    /// A device type that represents the GPU.
    /// The GPU device.
    #[doc(alias = "MLCDeviceTypeGPU")]
    pub const GPU: Self = Self(1);
    /// A device type that represents either the CPU or GPU.
    /// The any device type.  When selected, the framework will automatically use the appropriate devices to achieve the best
    /// performance.
    #[doc(alias = "MLCDeviceTypeAny")]
    pub const Any: Self = Self(2);
    /// A device type that represents the Apple Neural Engine.
    /// The  Apple Neural Engine device.  When selected, the framework will use the  Neural Engine to execute all layers that can be executed on it.
    /// Layers that cannot be executed on the ANE will run on the CPU or GPU.   The Neural Engine device must be explicitly selected.  MLDeviceTypeAny
    /// will not select the Neural Engine device.  In addition, this device can be used with inference graphs only.  This device cannot be used with a
    /// training graph or an inference graph that shares layers with a training graph.
    #[doc(alias = "MLCDeviceTypeANE")]
    pub const ANE: Self = Self(3);
    /// A number that represents the number of device types.
    /// The  Apple Neural Engine device.  When selected, the framework will use the  Neural Engine to execute all layers that can be executed on it.
    /// Layers that cannot be executed on the ANE will run on the CPU or GPU.   The Neural Engine device must be explicitly selected.  MLDeviceTypeAny
    /// will not select the Neural Engine device.  In addition, this device can be used with inference graphs only.  This device cannot be used with a
    /// training graph or an inference graph that shares layers with a training graph.
    #[doc(alias = "MLCDeviceTypeCount")]
    pub const Count: Self = Self(4);
}

unsafe impl Encode for MLCDeviceType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCDeviceType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A bitmask that specifies the options you use when compiling a graph.
/// A bitmask that specifies the options you use when compiling a graph.
///
/// This is passed as an argument to the compileWithOptions method avalable on MLCTrainingGraph and MLCInferenceGraph
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCGraphCompilationOptions(pub u64);
bitflags::bitflags! {
    impl MLCGraphCompilationOptions: u64 {
/// The default option for graph compilation.
/// No graph compilation options.
        #[doc(alias = "MLCGraphCompilationOptionsNone")]
        const None = 0x00;
/// The option to debug layers during graph compilation.
///
/// ## Discussion
///
/// Include this option to disable various optimizations such as layer fusion, and ensure the framework synchronizes the resulting forward and gradients tensors host memory with device memory, for layers marked as debuggable.
///
///
/// The option to debug layers during graph compilation.
///
///
/// Include this option to disable various optimizations such as layer fusion, and ensure the framework synchronizes
/// the resulting forward and gradients tensors host memory with device memory, for layers marked as debuggable.
        #[doc(alias = "MLCGraphCompilationOptionsDebugLayers")]
        const DebugLayers = 0x01;
/// The option to disable layer fusion during graph compilation.
///
/// ## Discussion
///
/// Include this option to disable fusion of layers, which is an important optimization that helps performance and memory footprint.
///
///
/// The option to disable layer fusion during graph compilation.
///
///
/// Include this option to disable fusion of layers, which is an important optimization that helps performance and
/// memory footprint.
        #[doc(alias = "MLCGraphCompilationOptionsDisableLayerFusion")]
        const DisableLayerFusion = 0x02;
/// The option to link graphs during graph compilation.
///
/// ## Discussion
///
/// Include this option when you link together one or more sub-graphs when executing the forward, gradient, and optimizer update. For example, if the full computation graph includes a layer that the framework doesn’t support, you’ll need to create multiple sub-graphs and link them together using `linkWithGraphs`. When doing so, include this option when you call `compileWithOptions` for graphs you want to link together.
///
///
/// The option to link graphs during graph compilation.
///
///
/// Include this option when you link together one or more sub-graphs when executing the forward, gradient, and
/// optimizer update. For example, if the full computation graph includes a layer that the framework doesn’t support, you’ll
/// need to create multiple sub-graphs and link them together using
/// `MLCGraphCompilationOptionsLinkGraphs.`When doing so,
/// include this option when you call
/// `-compileWithOptions:`for graphs you want to link together.
        #[doc(alias = "MLCGraphCompilationOptionsLinkGraphs")]
        const LinkGraphs = 0x04;
/// The option to compute all gradients during graph compilation.
///
/// ## Discussion
///
/// Include this option to compute gradients for layers with or without parameters that only take input tensors.
///
/// For example, if the first layer of a graph is a convolution layer, the framework only computes the gradients for weights and biases associated with the convolution layer, but not the gradients for the input. Include this option if you want to compute all gradients for the input.
///
///
/// The option to compute all gradients during graph compilation.
///
///
/// Include this option to compute gradients for layers with or without parameters that only take input tensors.
/// For example, if the first layer of a graph is a convolution layer, the framework only computes the gradients for weights
/// and biases associated with the convolution layer, but not the gradients for the input. Include this option if you want to
/// compute all gradients for the input.
        #[doc(alias = "MLCGraphCompilationOptionsComputeAllGradients")]
        const ComputeAllGradients = 0x08;
    }
}

unsafe impl Encode for MLCGraphCompilationOptions {
    const ENCODING: Encoding = u64::ENCODING;
}

unsafe impl RefEncode for MLCGraphCompilationOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A bitmask that specifies the options you use when executing a graph.
/// A bitmask that specifies the options you’ll use when executing a graph.
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCExecutionOptions(pub u64);
bitflags::bitflags! {
    impl MLCExecutionOptions: u64 {
/// The option to execute the graph in the most efficient way possible.
        #[doc(alias = "MLCExecutionOptionsNone")]
        const None = 0x00;
/// The option to skip writing input data to device memory.
///
/// ## Discussion
///
/// Include this option to prevent writing the input tensors to device memory associated with these tensors when the framework executes the graph.
///
///
/// The option to skip writing input data to device memory.
///
///
/// this option to prevent writing the input tensors to device memory associated with these tensors when the framework
/// executes the graph.
        #[doc(alias = "MLCExecutionOptionsSkipWritingInputDataToDevice")]
        const SkipWritingInputDataToDevice = 0x01;
/// The option to execute the graph synchronously.
///
/// ## Discussion
///
/// Include this option to wait until execution of the graph on specified device finishes before returning from the execute method.
///
///
/// The option to execute the graph synchronously.
///
///
/// Include this option to wait until execution of the graph on specified device finishes before returning from the
/// `execute`method.
        #[doc(alias = "MLCExecutionOptionsSynchronous")]
        const Synchronous = 0x02;
/// The option to return profiling information in the callback before returning from execution.
///
/// ## Discussion
///
/// Include this option to return profiling information in the graph execute completion handler callback, including device execution time.
///
///
/// The option to return profiling information in the callback before returning from execution.
///
///
/// Include this option to return profliling information in the graph execute completion handler callback, including
/// device execution time.
        #[doc(alias = "MLCExecutionOptionsProfiling")]
        const Profiling = 0x04;
/// The option to execute the forward pass for inference only.
///
/// ## Discussion
///
/// If you include this option and execute a training graph using one of the `execute` methods, such as [`executeWithInputsData:lossLabelsData:lossLabelWeightsData:batchSize:options:completionHandler:`](https://developer.apple.com/documentation/mlcompute/mlctraininggraph/execute(inputsdata:losslabelsdata:losslabelweightsdata:batchsize:options:completionhandler:)), the framework only executes the forward pass of the training graph, and it executes that forward pass for inference only.
///
/// If you include this option and execute a training graph using one of the `executeForward` methods, such as [`executeForwardWithBatchSize:options:completionHandler:`](https://developer.apple.com/documentation/mlcompute/mlctraininggraph/executeforward(batchsize:options:completionhandler:)), the framework executes the forward pass for inference only.
///
///
/// The option to execute the forward pass for inference only.
///
///
/// If you include this option and execute a training graph using one of the
/// `execute`methods, such as
/// `-executeWithInputsData:lossLabelsData:lossLabelWeightsData:batchSize:options:completionHandler:`, the framework only
/// executes the forward pass of the training graph, and it executes that forward pass for inference only.
///
/// If you include this option and execute a training graph using one of the executeForward methods, such as
/// `-executeForwardWithBatchSize:options:completionHandler:),`the framework executes the forward pass for inference only.
        #[doc(alias = "MLCExecutionOptionsForwardForInference")]
        const ForwardForInference = 0x08;
/// The option to enable additional per-layer profiling information using signposts.
///
/// ## Discussion
///
/// Visualize the layer information using the Logging profiling template in Instruments. This information may not be available for all [`ML Compute`](https://developer.apple.com/documentation/mlcompute) devices.
///
///
/// The option to enable additional per layer profiling information currently emitted using signposts.
///
///
/// The option to enable per layer profiling information emitted as signposts. The per layer information
/// can be visualized using the Logging Instrument in Xcode's Instruments. This information may not be available for all MLCDevice.
        #[doc(alias = "MLCExecutionOptionsPerLayerProfiling")]
        const PerLayerProfiling = 0x10;
    }
}

unsafe impl Encode for MLCExecutionOptions {
    const ENCODING: Encoding = u64::ENCODING;
}

unsafe impl RefEncode for MLCExecutionOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Constants that describe an arithmetic operation.
/// The list of supported arithmetic operations.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCArithmeticOperation(pub i32);
impl MLCArithmeticOperation {
    /// Calculates the element-wise sum of the inputs.
    /// An operation that calculates the elementwise sum of its two inputs.
    #[doc(alias = "MLCArithmeticOperationAdd")]
    pub const Add: Self = Self(0);
    /// Calculates the element-wise difference between the inputs.
    /// An operation that calculates the elementwise difference of its two inputs.
    #[doc(alias = "MLCArithmeticOperationSubtract")]
    pub const Subtract: Self = Self(1);
    /// Calculates the element-wise product of the inputs.
    /// An operation that calculates the elementwise product of its two inputs.
    #[doc(alias = "MLCArithmeticOperationMultiply")]
    pub const Multiply: Self = Self(2);
    /// Calculates the element-wise division of the inputs.
    /// An operation that calculates the elementwise division of its two inputs.
    #[doc(alias = "MLCArithmeticOperationDivide")]
    pub const Divide: Self = Self(3);
    /// Calculates the element-wise floor of the inputs.
    /// An operation that calculates the elementwise floor of its two inputs.
    #[doc(alias = "MLCArithmeticOperationFloor")]
    pub const Floor: Self = Self(4);
    /// Calculates the element-wise rounding of the inputs.
    /// An operation that calculates the elementwise round of its inputs.
    #[doc(alias = "MLCArithmeticOperationRound")]
    pub const Round: Self = Self(5);
    /// Calculates the element-wise ceiling of the inputs.
    /// An operation that calculates the elementwise ceiling of its inputs.
    #[doc(alias = "MLCArithmeticOperationCeil")]
    pub const Ceil: Self = Self(6);
    /// Calculates the element-wise square root of the input.
    /// An operation that calculates the elementwise square root of its inputs.
    #[doc(alias = "MLCArithmeticOperationSqrt")]
    pub const Sqrt: Self = Self(7);
    /// Calculates the element-wise reciprocal of the square root of the input.
    /// An operation that calculates the elementwise reciprocal of the square root of its inputs.
    #[doc(alias = "MLCArithmeticOperationRsqrt")]
    pub const Rsqrt: Self = Self(8);
    /// Calculates the element-wise sine of the input.
    /// An operation that calculates the elementwise sine of its inputs.
    #[doc(alias = "MLCArithmeticOperationSin")]
    pub const Sin: Self = Self(9);
    /// Calculates the element-wise cosine of the input.
    /// An operation that calculates the elementwise cosine of its inputs.
    #[doc(alias = "MLCArithmeticOperationCos")]
    pub const Cos: Self = Self(10);
    /// Calculates the element-wise tangent of the input.
    /// An operation that calculates the elementwise tangent of its inputs.
    #[doc(alias = "MLCArithmeticOperationTan")]
    pub const Tan: Self = Self(11);
    /// Calculates the element-wise inverse sine of the input.
    /// An operation that calculates the elementwise inverse sine of its inputs.
    #[doc(alias = "MLCArithmeticOperationAsin")]
    pub const Asin: Self = Self(12);
    /// Calculates the element-wise inverse cosine of the input.
    /// An operation that calculates the elementwise inverse cosine of its inputs.
    #[doc(alias = "MLCArithmeticOperationAcos")]
    pub const Acos: Self = Self(13);
    /// Calculates the element-wise inverse tangent of the input.
    /// An operation that calculates the elementwise inverse tangent of its inputs.
    #[doc(alias = "MLCArithmeticOperationAtan")]
    pub const Atan: Self = Self(14);
    /// Calculates the element-wise hyperbolic sine of the input.
    /// An operation that calculates the elementwise hyperbolic sine of its inputs.
    #[doc(alias = "MLCArithmeticOperationSinh")]
    pub const Sinh: Self = Self(15);
    /// Calculates the element-wise hyperbolic cosine of the input.
    /// An operation that calculates the elementwise hyperbolic cosine of its inputs.
    #[doc(alias = "MLCArithmeticOperationCosh")]
    pub const Cosh: Self = Self(16);
    /// Calculates the element-wise hyperbolic tangent of the input.
    /// An operation that calculates the elementwise hyperbolic tangent of its inputs.
    #[doc(alias = "MLCArithmeticOperationTanh")]
    pub const Tanh: Self = Self(17);
    /// Calculates the element-wise inverse hyperbolic sine of the input.
    /// An operation that calculates the elementwise inverse hyperbolic sine of its inputs.
    #[doc(alias = "MLCArithmeticOperationAsinh")]
    pub const Asinh: Self = Self(18);
    /// Calculates the element-wise inverse hyperbolic cosine of the input.
    /// An operation that calculates the elementwise inverse hyperbolic cosine of its inputs.
    #[doc(alias = "MLCArithmeticOperationAcosh")]
    pub const Acosh: Self = Self(19);
    /// Calculates the element-wise inverse hyperbolic tangent of the input.
    /// An operation that calculates the elementwise inverse hyperbolic tangent of its inputs.
    #[doc(alias = "MLCArithmeticOperationAtanh")]
    pub const Atanh: Self = Self(20);
    /// Calculates the element-wise first input raised to the power of the second input.
    /// An operation that calculates the elementwise first input raised to the power of its second input.
    #[doc(alias = "MLCArithmeticOperationPow")]
    pub const Pow: Self = Self(21);
    /// Calculates the element-wise result of the exponent raised to the power of the input.
    /// An operation that calculates the elementwise result of e raised to the power of its input.
    #[doc(alias = "MLCArithmeticOperationExp")]
    pub const Exp: Self = Self(22);
    /// Calculates the element-wise result of the number 2 raised to the power of the input.
    /// An operation that calculates the elementwise result of 2 raised to the power of its input.
    #[doc(alias = "MLCArithmeticOperationExp2")]
    pub const Exp2: Self = Self(23);
    /// Calculates the element-wise natural logarithm of the input.
    /// An operation that calculates the elementwise natural logarithm of its input.
    #[doc(alias = "MLCArithmeticOperationLog")]
    pub const Log: Self = Self(24);
    /// Calculates the element-wise base 2 logarithm of the input.
    /// An operation that calculates the elementwise base 2 logarithm of its input.
    #[doc(alias = "MLCArithmeticOperationLog2")]
    pub const Log2: Self = Self(25);
    /// Calculates the element-wise product of the inputs, and returns `0` when the result isn’t a number or infinity.
    ///
    /// ## Discussion
    ///
    /// Returns `0` if `y` in `x * y` is zero, even if `x` isn’t a number (`NaN)` or infinity (`INF)`.
    ///
    ///
    /// An operation that calculates the elementwise product of its two inputs.  Returns 0 if y in x * y is zero, even if x is NaN or INF
    #[doc(alias = "MLCArithmeticOperationMultiplyNoNaN")]
    pub const MultiplyNoNaN: Self = Self(26);
    /// Calculates the element-wise division of the inputs, and returns `0` if the denominator is `0`.
    ///
    /// ## Discussion
    ///
    /// Returns `0` if the denominator is `0`.
    ///
    ///
    /// An operations that calculates the elementwise division of its two inputs.  Returns 0 if the denominator is 0.
    #[doc(alias = "MLCArithmeticOperationDivideNoNaN")]
    pub const DivideNoNaN: Self = Self(27);
    /// Calculates the element-wise minimum of the inputs.
    /// An operation that calculates the elementwise min of two inputs.
    #[doc(alias = "MLCArithmeticOperationMin")]
    pub const Min: Self = Self(28);
    /// Calculates the element-wise maximum the inputs.
    /// An operations that calculates the elementwise max of two inputs.
    #[doc(alias = "MLCArithmeticOperationMax")]
    pub const Max: Self = Self(29);
    /// The total number of arithmetic operations.
    /// An operations that calculates the elementwise max of two inputs.
    #[doc(alias = "MLCArithmeticOperationCount")]
    pub const Count: Self = Self(30);
}

unsafe impl Encode for MLCArithmeticOperation {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCArithmeticOperation {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A loss function.
/// A loss function.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCLossType(pub i32);
impl MLCLossType {
    /// The mean absolute error loss.
    #[doc(alias = "MLCLossTypeMeanAbsoluteError")]
    pub const MeanAbsoluteError: Self = Self(0);
    /// The mean squared error loss.
    #[doc(alias = "MLCLossTypeMeanSquaredError")]
    pub const MeanSquaredError: Self = Self(1);
    /// The softmax cross entropy loss.
    #[doc(alias = "MLCLossTypeSoftmaxCrossEntropy")]
    pub const SoftmaxCrossEntropy: Self = Self(2);
    /// The sigmoid cross entropy loss.
    #[doc(alias = "MLCLossTypeSigmoidCrossEntropy")]
    pub const SigmoidCrossEntropy: Self = Self(3);
    /// The categorical cross entropy loss.
    #[doc(alias = "MLCLossTypeCategoricalCrossEntropy")]
    pub const CategoricalCrossEntropy: Self = Self(4);
    /// The hinge loss.
    #[doc(alias = "MLCLossTypeHinge")]
    pub const Hinge: Self = Self(5);
    /// The Huber loss.
    #[doc(alias = "MLCLossTypeHuber")]
    pub const Huber: Self = Self(6);
    /// The cosine distance loss.
    #[doc(alias = "MLCLossTypeCosineDistance")]
    pub const CosineDistance: Self = Self(7);
    /// The log loss.
    #[doc(alias = "MLCLossTypeLog")]
    pub const Log: Self = Self(8);
    /// The log loss.
    #[doc(alias = "MLCLossTypeCount")]
    pub const Count: Self = Self(9);
}

unsafe impl Encode for MLCLossType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCLossType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// An activation type that you specify for an activation descriptor.
/// An activation type that you specify for an activation descriptor.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCActivationType(pub i32);
impl MLCActivationType {
    /// An activation type that implements the identity function.
    /// The identity activation type.
    #[doc(alias = "MLCActivationTypeNone")]
    pub const None: Self = Self(0);
    /// An activation type that implements the rectified linear unit activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x >= 0 ? x : a * x`
    ///
    ///
    /// The ReLU activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x >= 0 ? x : a * x
    /// ```
    #[doc(alias = "MLCActivationTypeReLU")]
    pub const ReLU: Self = Self(1);
    /// An activation type that implements the linear activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = a * x + b`.
    ///
    ///
    /// The linear activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = a * x + b
    /// ```
    #[doc(alias = "MLCActivationTypeLinear")]
    pub const Linear: Self = Self(2);
    /// An activation type that implements the sigmoid activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = 1 / (1 + e⁻ˣ)`.
    ///
    ///
    /// The sigmoid activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = 1 / (1 + e⁻ˣ)
    /// ```
    #[doc(alias = "MLCActivationTypeSigmoid")]
    pub const Sigmoid: Self = Self(3);
    /// An activation type that implements the hard sigmoid activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = clamp((x * a) + b, 0, 1)`
    ///
    ///
    /// The hard sigmoid activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = clamp((x * a) + b, 0, 1)
    /// ```
    #[doc(alias = "MLCActivationTypeHardSigmoid")]
    pub const HardSigmoid: Self = Self(4);
    /// An activation type that implements the hyperbolic tangent activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the function:
    ///
    /// `f(x) = a * tanh(b * x)`
    ///
    ///
    /// The hyperbolic tangent (TanH) activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = a * tanh(b * x)
    /// ```
    #[doc(alias = "MLCActivationTypeTanh")]
    pub const Tanh: Self = Self(5);
    /// An activation type that implements the absolute activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = fabs(x)`
    ///
    ///
    /// The absolute activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = fabs(x)
    /// ```
    #[doc(alias = "MLCActivationTypeAbsolute")]
    pub const Absolute: Self = Self(6);
    /// An activation type that implements the soft plus activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = a * log(1 + e^(b * x))`
    ///
    ///
    /// The parametric soft plus activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = a * log(1 + e^(b * x))
    /// ```
    #[doc(alias = "MLCActivationTypeSoftPlus")]
    pub const SoftPlus: Self = Self(7);
    /// An activation type that implements the parametric soft sign activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the function:
    ///
    /// `f(x) = x / (1 + abs(x))`
    ///
    ///
    /// The parametric soft sign activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x / (1 + abs(x))
    ///   \endcod
    ///      
    ///
    /// ```
    #[doc(alias = "MLCActivationTypeSoftSign")]
    pub const SoftSign: Self = Self(8);
    /// An activation type that implements the exponential linear unit activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x >= 0 ? x : a * (exp(x) - 1)`
    ///
    ///
    /// The parametric ELU activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x >= 0 ? x : a * (exp(x) - 1)
    /// ```
    #[doc(alias = "MLCActivationTypeELU")]
    pub const ELU: Self = Self(9);
    /// An activation type that implements the ReLUN activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = min((x >= 0 ? x : a * x), b)`
    ///
    ///
    /// The ReLUN activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = min((x >= 0 ? x : a * x), b)
    /// ```
    #[doc(alias = "MLCActivationTypeReLUN")]
    pub const ReLUN: Self = Self(10);
    /// An activation type that implements the log sigmoid activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = log(1 / (1 + exp(-x)))`
    ///
    ///
    /// The log sigmoid activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = log(1 / (1 + exp(-x)))
    /// ```
    #[doc(alias = "MLCActivationTypeLogSigmoid")]
    pub const LogSigmoid: Self = Self(11);
    /// An activation type that implements the scaled exponential linear unit activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = scale * (max(0, x) + min(0, α * (exp(x)−1)))`, where:
    ///
    /// `α = 1.6732632423543772848170429916717`
    ///
    /// `scale = 1.0507009873554804934193349852946`
    ///
    ///
    /// The SELU activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = scale * (max(0, x) + min(0, α * (exp(x) − 1)))
    /// ```
    ///
    /// where:
    ///
    /// ```text
    ///   α = 1.6732632423543772848170429916717
    ///   scale = 1.0507009873554804934193349852946
    /// ```
    #[doc(alias = "MLCActivationTypeSELU")]
    pub const SELU: Self = Self(12);
    /// An activation type that implements the CELU activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = max(0, x) + min(0, a * (exp(x / a) − 1))`
    ///
    ///
    /// The CELU activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = max(0, x) + min(0, a * (exp(x / a) − 1))
    /// ```
    #[doc(alias = "MLCActivationTypeCELU")]
    pub const CELU: Self = Self(13);
    /// An activation type that implements the hard shrink activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x`, if `x > a` or `x < −a`, else `0`
    ///
    ///
    /// The hard shrink activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x, if x > a or x < −a, else 0
    /// ```
    #[doc(alias = "MLCActivationTypeHardShrink")]
    pub const HardShrink: Self = Self(14);
    /// An activation type that implements the soft shrink activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x - a`, if `x > a, x + a`, if `x < −a`, else `0`
    ///
    ///
    /// The soft shrink activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x - a, if x > a, x + a, if x < −a, else 0
    /// ```
    #[doc(alias = "MLCActivationTypeSoftShrink")]
    pub const SoftShrink: Self = Self(15);
    /// An activation type that implements the hyperbolic tangent shrink activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the function `f(x) = x - tanh(x)`.
    ///
    ///
    /// The hyperbolic tangent (TanH) shrink activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x - tanh(x)
    /// ```
    #[doc(alias = "MLCActivationTypeTanhShrink")]
    pub const TanhShrink: Self = Self(16);
    /// An activation type that implements the threshold activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x`, if `x > a`, else `b`, where:
    ///
    /// `a = threshold`
    ///
    /// `b = replacement`
    ///
    ///
    /// The threshold activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x, if x > a, else b
    /// ```
    #[doc(alias = "MLCActivationTypeThreshold")]
    pub const Threshold: Self = Self(17);
    /// An activation type that implements the gaussian error linear unit activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = x * CDF(x)`
    ///
    ///
    /// The GELU activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = x * CDF(x)
    /// ```
    #[doc(alias = "MLCActivationTypeGELU")]
    pub const GELU: Self = Self(18);
    /// An activation type that implements the hard swish activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = 0, if x <= -3`
    ///
    /// `f(x) = x, if x >= +3`
    ///
    /// `f(x) = x * (x + 3)/6`
    ///
    ///
    /// The hardswish activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = 0, if x <= -3
    ///   f(x) = x, if x >= +3
    ///   f(x) = x * (x + 3)/6, otherwise
    /// ```
    #[doc(alias = "MLCActivationTypeHardSwish")]
    pub const HardSwish: Self = Self(19);
    /// An activation type that implements the clamp activation function.
    ///
    /// ## Discussion
    ///
    /// This activation type implements the following function:
    ///
    /// `f(x) = min(max(x, a), b)`
    ///
    ///
    /// The clamp activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = min(max(x, a), b)
    /// ```
    #[doc(alias = "MLCActivationTypeClamp")]
    pub const Clamp: Self = Self(20);
    /// The count of activation types.
    /// The clamp activation type.
    ///
    /// This activation type implements the following function:
    ///
    /// ```text
    ///   f(x) = min(max(x, a), b)
    /// ```
    #[doc(alias = "MLCActivationTypeCount")]
    pub const Count: Self = Self(21);
}

unsafe impl Encode for MLCActivationType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCActivationType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// The convolution type specified for a convolution layer.
/// A convolution type that you specify for a convolution descriptor.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCConvolutionType(pub i32);
impl MLCConvolutionType {
    /// The standard convolution type.
    #[doc(alias = "MLCConvolutionTypeStandard")]
    pub const Standard: Self = Self(0);
    /// The transposed convolution type.
    #[doc(alias = "MLCConvolutionTypeTransposed")]
    pub const Transposed: Self = Self(1);
    /// The depthwise convolution type.
    #[doc(alias = "MLCConvolutionTypeDepthwise")]
    pub const Depthwise: Self = Self(2);
}

unsafe impl Encode for MLCConvolutionType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCConvolutionType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A padding policy that you specify for a convolution or pooling layer.
/// A padding policy that you specify for a convolution or pooling layer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCPaddingPolicy(pub i32);
impl MLCPaddingPolicy {
    /// The "same" padding policy.
    #[doc(alias = "MLCPaddingPolicySame")]
    pub const Same: Self = Self(0);
    /// The "valid" padding policy.
    #[doc(alias = "MLCPaddingPolicyValid")]
    pub const Valid: Self = Self(1);
    /// The choice to use explicitly specified padding sizes.
    #[doc(alias = "MLCPaddingPolicyUsePaddingSize")]
    pub const UsePaddingSize: Self = Self(2);
}

unsafe impl Encode for MLCPaddingPolicy {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCPaddingPolicy {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A padding type that you specify for a padding layer.
/// A padding type that you specify for a padding layer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCPaddingType(pub i32);
impl MLCPaddingType {
    /// The zero padding type.
    #[doc(alias = "MLCPaddingTypeZero")]
    pub const Zero: Self = Self(0);
    /// The reflect padding type.
    #[doc(alias = "MLCPaddingTypeReflect")]
    pub const Reflect: Self = Self(1);
    /// The symmetric padding type.
    #[doc(alias = "MLCPaddingTypeSymmetric")]
    pub const Symmetric: Self = Self(2);
    /// The constant padding type.
    #[doc(alias = "MLCPaddingTypeConstant")]
    pub const Constant: Self = Self(3);
}

unsafe impl Encode for MLCPaddingType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCPaddingType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A pooling function type for a pooling layer.
/// A pooling function type for a pooling layer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct MLCPoolingType(pub i32);
impl MLCPoolingType {
    /// The max pooling type.
    #[doc(alias = "MLCPoolingTypeMax")]
    pub const Max: Self = Self(1);
    /// The average pooling type.
    #[doc(alias = "MLCPoolingTypeAverage")]
    pub const Average: Self = Self(2);
    /// The L2-norm pooling type.
    #[doc(alias = "MLCPoolingTypeL2Norm")]
    pub const L2Norm: Self = Self(3);
    /// The L2-norm pooling type.
    #[doc(alias = "MLCPoolingTypeCount")]
    pub const Count: Self = Self(4);
}

unsafe impl Encode for MLCPoolingType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCPoolingType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Constants that describe a reduction operation type.
/// A reduction operation type.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCReductionType(pub i32);
impl MLCReductionType {
    /// A reduction operation that applies no reduction.
    /// No reduction.
    #[doc(alias = "MLCReductionTypeNone")]
    pub const None: Self = Self(0);
    /// A reduction operation that applies to the sum of the dimensions.
    /// The sum reduction.
    #[doc(alias = "MLCReductionTypeSum")]
    pub const Sum: Self = Self(1);
    /// A reduction operation that applies to the mean of the dimensions.
    /// The mean reduction.
    #[doc(alias = "MLCReductionTypeMean")]
    pub const Mean: Self = Self(2);
    /// A reduction operation that applies to the maximum dimension.
    /// The max reduction.
    #[doc(alias = "MLCReductionTypeMax")]
    pub const Max: Self = Self(3);
    /// A reduction operation that applies to the minimum dimension.
    /// The min reduction.
    #[doc(alias = "MLCReductionTypeMin")]
    pub const Min: Self = Self(4);
    /// A reduction operation that applies to the maximum dimension you specify.
    /// The argmax reduction.
    #[doc(alias = "MLCReductionTypeArgMax")]
    pub const ArgMax: Self = Self(5);
    /// A reduction operation that applies to the minimum dimension you specify.
    /// The argmin reduction.
    #[doc(alias = "MLCReductionTypeArgMin")]
    pub const ArgMin: Self = Self(6);
    /// A reduction operation that applies a lasso regularization penalty.
    /// The L1norm reduction.
    #[doc(alias = "MLCReductionTypeL1Norm")]
    pub const L1Norm: Self = Self(7);
    /// A reduction operation that applies to any dimension.
    /// Any(X) = X_0 || X_1 || ... X_n
    #[doc(alias = "MLCReductionTypeAny")]
    pub const Any: Self = Self(8);
    /// A reduction operation that applies to all dimensions.
    /// Alf(X) = X_0
    /// &
    /// &
    /// X_1
    /// &
    /// &
    /// ... X_n
    #[doc(alias = "MLCReductionTypeAll")]
    pub const All: Self = Self(9);
    /// The total number of reduction operations.
    /// Alf(X) = X_0
    /// &
    /// &
    /// X_1
    /// &
    /// &
    /// ... X_n
    #[doc(alias = "MLCReductionTypeCount")]
    pub const Count: Self = Self(10);
}

unsafe impl Encode for MLCReductionType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCReductionType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A regularization function to use with an optimizer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCRegularizationType(pub i32);
impl MLCRegularizationType {
    /// No regularization.
    #[doc(alias = "MLCRegularizationTypeNone")]
    pub const None: Self = Self(0);
    /// The L1 regularization.
    #[doc(alias = "MLCRegularizationTypeL1")]
    pub const L1: Self = Self(1);
    /// The L2 regularization.
    #[doc(alias = "MLCRegularizationTypeL2")]
    pub const L2: Self = Self(2);
}

unsafe impl Encode for MLCRegularizationType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCRegularizationType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A sampling mode for an upsample layer.
/// A sampling mode for an upsample layer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCSampleMode(pub i32);
impl MLCSampleMode {
    /// The nearest sample mode.
    #[doc(alias = "MLCSampleModeNearest")]
    pub const Nearest: Self = Self(0);
    /// The linear sample mode.
    #[doc(alias = "MLCSampleModeLinear")]
    pub const Linear: Self = Self(1);
}

unsafe impl Encode for MLCSampleMode {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCSampleMode {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A softmax operation.
/// A softmax operation.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCSoftmaxOperation(pub i32);
impl MLCSoftmaxOperation {
    /// The standard softmax operation.
    #[doc(alias = "MLCSoftmaxOperationSoftmax")]
    pub const Softmax: Self = Self(0);
    /// The log softmax operation.
    #[doc(alias = "MLCSoftmaxOperationLogSoftmax")]
    pub const LogSoftmax: Self = Self(1);
}

unsafe impl Encode for MLCSoftmaxOperation {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCSoftmaxOperation {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Constants that describe the result of an LSTM layer.
/// A result mode for an LSTM layer.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCLSTMResultMode(pub u64);
impl MLCLSTMResultMode {
    /// A result mode that indicates the layer produces a single result tensor that represents the final output of the LSTM.
    /// The output result mode. When selected for an LSTM layer, the layer will produce a single result tensor representing the final output of the LSTM.
    #[doc(alias = "MLCLSTMResultModeOutput")]
    pub const Output: Self = Self(0);
    /// A result mode that indicates the layer produces three result tensors that represent the final output of the LSTM, the last hidden state, and the cell state.
    /// The output and states result mode. When selected for an LSTM layer, the layer will produce three result tensors representing the final output of
    /// the LSTM, the last hidden state, and the cell state, respectively.
    #[doc(alias = "MLCLSTMResultModeOutputAndStates")]
    pub const OutputAndStates: Self = Self(1);
}

unsafe impl Encode for MLCLSTMResultMode {
    const ENCODING: Encoding = u64::ENCODING;
}

unsafe impl RefEncode for MLCLSTMResultMode {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A comparison operation.
/// A comparison operation.
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCComparisonOperation(pub i32);
impl MLCComparisonOperation {
    #[doc(alias = "MLCComparisonOperationEqual")]
    pub const Equal: Self = Self(0);
    #[doc(alias = "MLCComparisonOperationNotEqual")]
    pub const NotEqual: Self = Self(1);
    #[doc(alias = "MLCComparisonOperationLess")]
    pub const Less: Self = Self(2);
    #[doc(alias = "MLCComparisonOperationGreater")]
    pub const Greater: Self = Self(3);
    #[doc(alias = "MLCComparisonOperationLessOrEqual")]
    pub const LessOrEqual: Self = Self(4);
    #[doc(alias = "MLCComparisonOperationGreaterOrEqual")]
    pub const GreaterOrEqual: Self = Self(5);
    #[doc(alias = "MLCComparisonOperationLogicalAND")]
    pub const LogicalAND: Self = Self(6);
    #[doc(alias = "MLCComparisonOperationLogicalOR")]
    pub const LogicalOR: Self = Self(7);
    #[doc(alias = "MLCComparisonOperationLogicalNOT")]
    pub const LogicalNOT: Self = Self(8);
    #[doc(alias = "MLCComparisonOperationLogicalNAND")]
    pub const LogicalNAND: Self = Self(9);
    #[doc(alias = "MLCComparisonOperationLogicalNOR")]
    pub const LogicalNOR: Self = Self(10);
    #[doc(alias = "MLCComparisonOperationLogicalXOR")]
    pub const LogicalXOR: Self = Self(11);
    /// A number that represents the operation count.
    #[doc(alias = "MLCComparisonOperationCount")]
    pub const Count: Self = Self(12);
}

unsafe impl Encode for MLCComparisonOperation {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCComparisonOperation {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A clipping type the system applies to a gradient.
/// The type of clipping applied to gradient
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, Default)]
pub struct MLCGradientClippingType(pub i32);
impl MLCGradientClippingType {
    /// An option that clips by value.
    #[doc(alias = "MLCGradientClippingTypeByValue")]
    pub const ByValue: Self = Self(0);
    /// An option that clips by norm.
    #[doc(alias = "MLCGradientClippingTypeByNorm")]
    pub const ByNorm: Self = Self(1);
    /// An option that clips by global norm.
    #[doc(alias = "MLCGradientClippingTypeByGlobalNorm")]
    pub const ByGlobalNorm: Self = Self(2);
}

unsafe impl Encode for MLCGradientClippingType {
    const ENCODING: Encoding = i32::ENCODING;
}

unsafe impl RefEncode for MLCGradientClippingType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

impl MLCActivationType {
    /// A textual description of the activation type, suitable for debugging.
    /// Returns a textual description of the activation type, suitable for debugging.
    #[doc(alias = "MLCActivationTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCActivationTypeDebugDescription(
                activation_type: MLCActivationType,
            ) -> *mut NSString;
        }
        let ret = unsafe { MLCActivationTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCArithmeticOperation {
    /// A textual description of the arithmetic operation you use for debugging.
    /// Returns a textual description of the arithmetic operation, suitable for debugging.
    #[doc(alias = "MLCArithmeticOperationDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCArithmeticOperationDebugDescription(
                operation: MLCArithmeticOperation,
            ) -> *mut NSString;
        }
        let ret = unsafe { MLCArithmeticOperationDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCPaddingPolicy {
    /// A textual description of the padding policy, suitable for debugging.
    /// Returns a textual description of the padding policy, suitable for debugging.
    #[doc(alias = "MLCPaddingPolicyDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCPaddingPolicyDebugDescription(padding_policy: MLCPaddingPolicy) -> *mut NSString;
        }
        let ret = unsafe { MLCPaddingPolicyDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCLossType {
    /// A textual description of the loss type, suitable for debugging.
    /// Returns a textual description of the loss type, suitable for debugging.
    #[doc(alias = "MLCLossTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCLossTypeDebugDescription(loss_type: MLCLossType) -> *mut NSString;
        }
        let ret = unsafe { MLCLossTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCReductionType {
    /// A textual description of the reduction operation you use for debugging.
    /// Returns a textual description of the reduction type, suitable for debugging.
    #[doc(alias = "MLCReductionTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCReductionTypeDebugDescription(reduction_type: MLCReductionType) -> *mut NSString;
        }
        let ret = unsafe { MLCReductionTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCPaddingType {
    /// A textual description of the padding type, suitable for debugging.
    /// Returns a textual description of the padding type, suitable for debugging.
    #[doc(alias = "MLCPaddingTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCPaddingTypeDebugDescription(padding_type: MLCPaddingType) -> *mut NSString;
        }
        let ret = unsafe { MLCPaddingTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCConvolutionType {
    /// A textual description of the convolution type, suitable for debugging.
    /// Returns a textual description of the convolution type, suitable for debugging.
    #[doc(alias = "MLCConvolutionTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCConvolutionTypeDebugDescription(
                convolution_type: MLCConvolutionType,
            ) -> *mut NSString;
        }
        let ret = unsafe { MLCConvolutionTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCPoolingType {
    /// A textual description of the pooling type, suitable for debugging.
    /// Returns a textual description of the pooling type, suitable for debugging.
    #[doc(alias = "MLCPoolingTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCPoolingTypeDebugDescription(pooling_type: MLCPoolingType) -> *mut NSString;
        }
        let ret = unsafe { MLCPoolingTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCSoftmaxOperation {
    /// A textual description of the softmax operation, suitable for debugging.
    /// Returns a textual description of the softmax operation, suitable for debugging.
    #[doc(alias = "MLCSoftmaxOperationDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCSoftmaxOperationDebugDescription(operation: MLCSoftmaxOperation)
                -> *mut NSString;
        }
        let ret = unsafe { MLCSoftmaxOperationDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCSampleMode {
    /// A textual description of the sample mode, suitable for debugging.
    /// Returns a textual description of the sample mode, suitable for debugging.
    #[doc(alias = "MLCSampleModeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCSampleModeDebugDescription(mode: MLCSampleMode) -> *mut NSString;
        }
        let ret = unsafe { MLCSampleModeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCLSTMResultMode {
    /// A textual description of the LSTM result mode you use for debugging.
    /// Returns a textual description of the LSTM result mode, suitable for debugging.
    #[doc(alias = "MLCLSTMResultModeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCLSTMResultModeDebugDescription(mode: MLCLSTMResultMode) -> *mut NSString;
        }
        let ret = unsafe { MLCLSTMResultModeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCComparisonOperation {
    /// A textual description of the comparison operation, suitable for debugging.
    /// Returns a textual description of the comparison operation, suitable for debugging.
    #[doc(alias = "MLCComparisonOperationDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCComparisonOperationDebugDescription(
                operation: MLCComparisonOperation,
            ) -> *mut NSString;
        }
        let ret = unsafe { MLCComparisonOperationDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

impl MLCGradientClippingType {
    /// A textual description of the gradient clipping type, suitable for debugging.
    /// Returns a textual description of the gradient clipping type, suitable for debugging.
    #[doc(alias = "MLCGradientClippingTypeDebugDescription")]
    #[inline]
    pub unsafe fn debug_description(self) -> Retained<NSString> {
        extern "C-unwind" {
            fn MLCGradientClippingTypeDebugDescription(
                gradient_clipping_type: MLCGradientClippingType,
            ) -> *mut NSString;
        }
        let ret = unsafe { MLCGradientClippingTypeDebugDescription(self) };
        unsafe { Retained::retain_autoreleased(ret) }
            .expect("function was marked as returning non-null, but actually returned NULL")
    }
}

#[deprecated = "renamed to `MLCActivationType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCActivationTypeDebugDescription(
    activation_type: MLCActivationType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCActivationTypeDebugDescription(activation_type: MLCActivationType) -> *mut NSString;
    }
    let ret = unsafe { MLCActivationTypeDebugDescription(activation_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCArithmeticOperation::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCArithmeticOperationDebugDescription(
    operation: MLCArithmeticOperation,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCArithmeticOperationDebugDescription(
            operation: MLCArithmeticOperation,
        ) -> *mut NSString;
    }
    let ret = unsafe { MLCArithmeticOperationDebugDescription(operation) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCPaddingPolicy::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCPaddingPolicyDebugDescription(
    padding_policy: MLCPaddingPolicy,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCPaddingPolicyDebugDescription(padding_policy: MLCPaddingPolicy) -> *mut NSString;
    }
    let ret = unsafe { MLCPaddingPolicyDebugDescription(padding_policy) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCLossType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCLossTypeDebugDescription(
    loss_type: MLCLossType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCLossTypeDebugDescription(loss_type: MLCLossType) -> *mut NSString;
    }
    let ret = unsafe { MLCLossTypeDebugDescription(loss_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCReductionType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCReductionTypeDebugDescription(
    reduction_type: MLCReductionType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCReductionTypeDebugDescription(reduction_type: MLCReductionType) -> *mut NSString;
    }
    let ret = unsafe { MLCReductionTypeDebugDescription(reduction_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCPaddingType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCPaddingTypeDebugDescription(
    padding_type: MLCPaddingType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCPaddingTypeDebugDescription(padding_type: MLCPaddingType) -> *mut NSString;
    }
    let ret = unsafe { MLCPaddingTypeDebugDescription(padding_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCConvolutionType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCConvolutionTypeDebugDescription(
    convolution_type: MLCConvolutionType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCConvolutionTypeDebugDescription(
            convolution_type: MLCConvolutionType,
        ) -> *mut NSString;
    }
    let ret = unsafe { MLCConvolutionTypeDebugDescription(convolution_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCPoolingType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCPoolingTypeDebugDescription(
    pooling_type: MLCPoolingType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCPoolingTypeDebugDescription(pooling_type: MLCPoolingType) -> *mut NSString;
    }
    let ret = unsafe { MLCPoolingTypeDebugDescription(pooling_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCSoftmaxOperation::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCSoftmaxOperationDebugDescription(
    operation: MLCSoftmaxOperation,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCSoftmaxOperationDebugDescription(operation: MLCSoftmaxOperation) -> *mut NSString;
    }
    let ret = unsafe { MLCSoftmaxOperationDebugDescription(operation) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCSampleMode::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCSampleModeDebugDescription(
    mode: MLCSampleMode,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCSampleModeDebugDescription(mode: MLCSampleMode) -> *mut NSString;
    }
    let ret = unsafe { MLCSampleModeDebugDescription(mode) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCLSTMResultMode::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCLSTMResultModeDebugDescription(
    mode: MLCLSTMResultMode,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCLSTMResultModeDebugDescription(mode: MLCLSTMResultMode) -> *mut NSString;
    }
    let ret = unsafe { MLCLSTMResultModeDebugDescription(mode) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCComparisonOperation::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCComparisonOperationDebugDescription(
    operation: MLCComparisonOperation,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCComparisonOperationDebugDescription(
            operation: MLCComparisonOperation,
        ) -> *mut NSString;
    }
    let ret = unsafe { MLCComparisonOperationDebugDescription(operation) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}

#[deprecated = "renamed to `MLCGradientClippingType::debug_description`"]
#[inline]
pub unsafe extern "C-unwind" fn MLCGradientClippingTypeDebugDescription(
    gradient_clipping_type: MLCGradientClippingType,
) -> Retained<NSString> {
    extern "C-unwind" {
        fn MLCGradientClippingTypeDebugDescription(
            gradient_clipping_type: MLCGradientClippingType,
        ) -> *mut NSString;
    }
    let ret = unsafe { MLCGradientClippingTypeDebugDescription(gradient_clipping_type) };
    unsafe { Retained::retain_autoreleased(ret) }
        .expect("function was marked as returning non-null, but actually returned NULL")
}
