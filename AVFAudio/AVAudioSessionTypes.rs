//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use objc2::__framework_prelude::*;
use objc2_foundation::*;

use crate::*;

/// A port describes a specific type of audio input or output device or connector.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionport?language=objc)
// NS_TYPED_ENUM
pub type AVAudioSessionPort = NSString;

extern "C" {
    /// Continuity microphone for appletv.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportcontinuitymicrophone?language=objc)
    pub static AVAudioSessionPortContinuityMicrophone: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Line level input on a dock connector
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportlinein?language=objc)
    pub static AVAudioSessionPortLineIn: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Built-in microphone on an iOS device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbuiltinmic?language=objc)
    pub static AVAudioSessionPortBuiltInMic: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Microphone on a wired headset.  Headset refers to an accessory that has headphone outputs paired with a
    /// microphone.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportheadsetmic?language=objc)
    pub static AVAudioSessionPortHeadsetMic: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Line level output on a dock connector
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportlineout?language=objc)
    pub static AVAudioSessionPortLineOut: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Headphone or headset output
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportheadphones?language=objc)
    pub static AVAudioSessionPortHeadphones: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Output on a Bluetooth A2DP device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbluetootha2dp?language=objc)
    pub static AVAudioSessionPortBluetoothA2DP: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// The speaker you hold to your ear when on a phone call
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbuiltinreceiver?language=objc)
    pub static AVAudioSessionPortBuiltInReceiver: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Built-in speaker on an iOS device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbuiltinspeaker?language=objc)
    pub static AVAudioSessionPortBuiltInSpeaker: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Output via High-Definition Multimedia Interface
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionporthdmi?language=objc)
    pub static AVAudioSessionPortHDMI: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Output on a remote Air Play device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportairplay?language=objc)
    pub static AVAudioSessionPortAirPlay: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Output on a Bluetooth Low Energy device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbluetoothle?language=objc)
    pub static AVAudioSessionPortBluetoothLE: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output on a Bluetooth Hands-Free Profile device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportbluetoothhfp?language=objc)
    pub static AVAudioSessionPortBluetoothHFP: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output on a Universal Serial Bus device
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportusbaudio?language=objc)
    pub static AVAudioSessionPortUSBAudio: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output via Car Audio
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportcaraudio?language=objc)
    pub static AVAudioSessionPortCarAudio: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output that does not correspond to real audio hardware
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportvirtual?language=objc)
    pub static AVAudioSessionPortVirtual: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output connected via the PCI (Peripheral Component Interconnect) bus
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportpci?language=objc)
    pub static AVAudioSessionPortPCI: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output connected via FireWire
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportfirewire?language=objc)
    pub static AVAudioSessionPortFireWire: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output connected via DisplayPort
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportdisplayport?language=objc)
    pub static AVAudioSessionPortDisplayPort: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output connected via AVB (Audio Video Bridging)
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportavb?language=objc)
    pub static AVAudioSessionPortAVB: Option<&'static AVAudioSessionPort>;
}

extern "C" {
    /// Input or output connected via Thunderbolt
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportthunderbolt?language=objc)
    pub static AVAudioSessionPortThunderbolt: Option<&'static AVAudioSessionPort>;
}

/// A category defines a broad set of behaviors for a session.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategory?language=objc)
// NS_TYPED_ENUM
pub type AVAudioSessionCategory = NSString;

extern "C" {
    /// Use this category for background sounds such as rain, car engine noise, etc.
    /// Mixes with other music.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryambient?language=objc)
    pub static AVAudioSessionCategoryAmbient: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category for background sounds.  Other music will stop playing.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategorysoloambient?language=objc)
    pub static AVAudioSessionCategorySoloAmbient: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category for music tracks.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryplayback?language=objc)
    pub static AVAudioSessionCategoryPlayback: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category when recording audio.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryrecord?language=objc)
    pub static AVAudioSessionCategoryRecord: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category when recording and playing back audio.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryplayandrecord?language=objc)
    pub static AVAudioSessionCategoryPlayAndRecord: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category when using a hardware codec or signal processor while
    /// not playing or recording audio.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryaudioprocessing?language=objc)
    #[deprecated = "No longer supported"]
    pub static AVAudioSessionCategoryAudioProcessing: Option<&'static AVAudioSessionCategory>;
}

extern "C" {
    /// Use this category to customize the usage of available audio accessories and built-in audio hardware.
    /// For example, this category provides an application with the ability to use an available USB output
    /// and headphone output simultaneously for separate, distinct streams of audio data. Use of
    /// this category by an application requires a more detailed knowledge of, and interaction with,
    /// the capabilities of the available audio routes.  May be used for input, output, or both.
    /// Note that not all output types and output combinations are eligible for multi-route.  Input is limited
    /// to the last-in input port. Eligible inputs consist of the following:
    /// AVAudioSessionPortUSBAudio, AVAudioSessionPortHeadsetMic, and AVAudioSessionPortBuiltInMic.
    /// Eligible outputs consist of the following:
    /// AVAudioSessionPortUSBAudio, AVAudioSessionPortLineOut, AVAudioSessionPortHeadphones, AVAudioSessionPortHDMI,
    /// and AVAudioSessionPortBuiltInSpeaker.
    /// Note that AVAudioSessionPortBuiltInSpeaker is only allowed to be used when there are no other eligible
    /// outputs connected.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategorymultiroute?language=objc)
    pub static AVAudioSessionCategoryMultiRoute: Option<&'static AVAudioSessionCategory>;
}

/// Modes modify the audio category in order to introduce behavior that is tailored to the specific
/// use of audio within an application.  Available in iOS 5.0 and greater.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmode?language=objc)
// NS_TYPED_ENUM
pub type AVAudioSessionMode = NSString;

extern "C" {
    /// The default mode
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodedefault?language=objc)
    pub static AVAudioSessionModeDefault: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Only valid with AVAudioSessionCategoryPlayAndRecord.  Appropriate for Voice over IP
    /// (VoIP) applications.  Reduces the number of allowable audio routes to be only those
    /// that are appropriate for VoIP applications and may engage appropriate system-supplied
    /// signal processing.  Has the side effect of setting AVAudioSessionCategoryOptionAllowBluetoothHFP.
    /// Using this mode without the VoiceProcessing IO unit or AVAudioEngine with voice processing enabled will result in the following:
    /// - Chat-specific signal processing such as echo cancellation or automatic gain correction will not be loaded
    /// - Dynamic processing on input and output will be disabled resulting in a lower output playback level.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodevoicechat?language=objc)
    pub static AVAudioSessionModeVoiceChat: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Set by Game Kit on behalf of an application that uses a GKVoiceChat object; valid
    /// only with the AVAudioSessionCategoryPlayAndRecord category.
    /// Do not set this mode directly. If you need similar behavior and are not using
    /// a GKVoiceChat object, use AVAudioSessionModeVoiceChat instead.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodegamechat?language=objc)
    pub static AVAudioSessionModeGameChat: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Only valid with AVAudioSessionCategoryPlayAndRecord or AVAudioSessionCategoryRecord.
    /// Modifies the audio routing options and may engage appropriate system-supplied signal processing.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodevideorecording?language=objc)
    pub static AVAudioSessionModeVideoRecording: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Appropriate for applications that wish to minimize the effect of system-supplied signal
    /// processing for input and/or output audio signals.
    /// This mode disables some dynamics processing on input and output resulting in a lower output playback level.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodemeasurement?language=objc)
    pub static AVAudioSessionModeMeasurement: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Appropriate for applications playing movie content. Only valid with AVAudioSessionCategoryPlayback.
    /// Setting this mode engages appropriate output signal processing for movie playback scenarios.
    /// Content using this mode is eligible for Enhance Dialogue processing on supported routes with capable hardware
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodemovieplayback?language=objc)
    pub static AVAudioSessionModeMoviePlayback: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Only valid with kAudioSessionCategory_PlayAndRecord. Reduces the number of allowable audio
    /// routes to be only those that are appropriate for video chat applications. May engage appropriate
    /// system-supplied signal processing.  Has the side effect of setting
    /// AVAudioSessionCategoryOptionAllowBluetoothHFP and AVAudioSessionCategoryOptionDefaultToSpeaker.
    /// Using this mode without the VoiceProcessing IO unit or AVAudioEngine with voice processing enabled will result in the following:
    /// - Chat-specific signal processing such as echo cancellation or automatic gain correction will not be loaded
    /// - Dynamic processing on input and output will be disabled resulting in a lower output playback level.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodevideochat?language=objc)
    pub static AVAudioSessionModeVideoChat: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Appropriate for applications which play spoken audio and wish to be paused (via audio session interruption) rather than ducked
    /// if another app (such as a navigation app) plays a spoken audio prompt.  Examples of apps that would use this are podcast players and
    /// audio books.  For more information, see the related category option AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodespokenaudio?language=objc)
    pub static AVAudioSessionModeSpokenAudio: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Appropriate for applications which play audio using text to speech. Setting this mode allows for different routing behaviors when
    /// connected to certain audio devices such as CarPlay. An example of an app that would use this mode is a turn by turn navigation app that
    /// plays short prompts to the user. Typically, these same types of applications would also configure their session to use
    /// AVAudioSessionCategoryOptionDuckOthers and AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodevoiceprompt?language=objc)
    pub static AVAudioSessionModeVoicePrompt: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Appropriate for applications playing short-form video content.
    ///
    /// Only valid with ``AVAudioSessionCategoryPlayback``.
    /// Not applicable with ``AVAudioSessionRouteSharingPolicy/AVAudioSessionRouteSharingPolicyLongFormAudio``,
    /// or ``AVAudioSessionRouteSharingPolicy/AVAudioSessionRouteSharingPolicyLongFormVideo``.
    ///
    /// When this mode is set:
    /// - system will make informed decisions to automatically unmute the output of the media if the user shows intention of unmuting.
    /// - When auto-unmuted, ``AVAudioSessionUserIntentToUnmuteOutputNotification`` and ``AVAudioSessionOutputMuteStateChangeNotification`` will be sent.
    /// - if the session is output muted, system may prevent interrupting other active audio apps.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmodeshortformvideo?language=objc)
    pub static AVAudioSessionModeShortFormVideo: Option<&'static AVAudioSessionMode>;
}

extern "C" {
    /// Notification sent to registered listeners when the system has interrupted the audio
    /// session and when the interruption has ended.
    ///
    /// Check the notification's userInfo dictionary for the interruption type, which is either
    /// Begin or End. In the case of an end interruption notification, check the userInfo dictionary
    /// for AVAudioSessionInterruptionOptions that indicate whether audio playback should resume.
    /// In the case of a begin interruption notification, the reason for the interruption can be found
    /// within the info dictionary under the key AVAudioSessionInterruptionReasonKey.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionnotification?language=objc)
    pub static AVAudioSessionInterruptionNotification: Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when an audio route change has occurred.
    ///
    /// Check the notification's userInfo dictionary for the route change reason and for a description
    /// of the previous audio route.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionroutechangenotification?language=objc)
    pub static AVAudioSessionRouteChangeNotification: Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners if the media server is killed.
    ///
    /// In the event that the server is killed, take appropriate steps to handle requests that come in
    /// before the server resets.  See Technical Q
    /// &A
    /// QA1749.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmediaserviceswerelostnotification?language=objc)
    pub static AVAudioSessionMediaServicesWereLostNotification: Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when the media server restarts.
    ///
    /// In the event that the server restarts, take appropriate steps to re-initialize any audio objects
    /// used by your application.  See Technical Q
    /// &A
    /// QA1749.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmediaserviceswereresetnotification?language=objc)
    pub static AVAudioSessionMediaServicesWereResetNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when they are in the foreground with an active
    /// audio session and primary audio from other applications starts and stops.
    ///
    /// Check the notification's userInfo dictionary for the notification type, which is either Begin or
    /// End. Foreground applications may use this notification as a hint to enable or disable audio that
    /// is secondary to the functionality of the application. For more information, see the related
    /// property secondaryAudioShouldBeSilencedHint.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionsilencesecondaryaudiohintnotification?language=objc)
    pub static AVAudioSessionSilenceSecondaryAudioHintNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when spatial playback capabilities are changed due to a
    /// change in user preference.
    ///
    /// Check the notification's userInfo dictionary for AVAudioSessionSpatialAudioEnabledKey to check if spatial
    /// audio is enabled.
    ///
    /// Observers of this notification should also observe AVAudioSessionRouteChangeNotification since a route change
    /// may also result in a change in the ability for the system to play spatial audio. Use
    /// AVAudioSessionPortDescription's isSpatialAudioEnabled property to check if the current route supports
    /// spatialized playback.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionspatialplaybackcapabilitieschangednotification?language=objc)
    pub static AVAudioSessionSpatialPlaybackCapabilitiesChangedNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when the resolved rendering mode changes.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionrenderingmodechangenotification?language=objc)
    pub static AVAudioSessionRenderingModeChangeNotification: Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when the rendering capabilities change.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionrenderingcapabilitieschangenotification?language=objc)
    pub static AVAudioSessionRenderingCapabilitiesChangeNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when the system's capability to inject audio into input stream is changed
    ///
    /// Check the notification's userInfo dictionary for AVAudioSessionMicrophoneInjectionIsAvailableKey to check if microphone
    /// injection is available. Use AVAudioSession's isMicrophoneInjectionAvailable property to check if microphone injection is available
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmicrophoneinjectioncapabilitieschangenotification?language=objc)
    pub static AVAudioSessionMicrophoneInjectionCapabilitiesChangeNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// Notification sent to registered listeners when session's output mute state changes.
    ///
    /// The userInfo dictionary will contain the updated output mute value as accessed by ``AVAudioSessionMuteStateKey``
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionoutputmutestatechangenotification?language=objc)
    pub static AVAudioSessionOutputMuteStateChangeNotification: Option<&'static NSNotificationName>;
}

extern "C" {
    /// Keys for ``AVAudioSessionOutputMuteStateChangeNotification``
    /// Value is `NSNumber` type with boolean value 0 for unmuted or value 1 for muted (samples zeroed out)
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmutestatekey?language=objc)
    pub static AVAudioSessionMuteStateKey: Option<&'static NSString>;
}

extern "C" {
    /// Notification sent to registered listeners when the application's output is muted and user hints to unmute.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionuserintenttounmuteoutputnotification?language=objc)
    pub static AVAudioSessionUserIntentToUnmuteOutputNotification:
        Option<&'static NSNotificationName>;
}

extern "C" {
    /// keys for AVAudioSessionSpatialPlaybackCapabilitiesChangedNotification
    /// value is an NSNumber whose boolean value indicates if spatial audio enabled.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionspatialaudioenabledkey?language=objc)
    pub static AVAudioSessionSpatialAudioEnabledKey: Option<&'static NSString>;
}

extern "C" {
    /// keys for AVAudioSessionInterruptionNotification
    /// Value is an NSNumber representing an AVAudioSessionInterruptionType
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptiontypekey?language=objc)
    pub static AVAudioSessionInterruptionTypeKey: Option<&'static NSString>;
}

extern "C" {
    /// Only present for end interruption events.  Value is of type AVAudioSessionInterruptionOptions.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionoptionkey?language=objc)
    pub static AVAudioSessionInterruptionOptionKey: Option<&'static NSString>;
}

extern "C" {
    /// Only present in begin interruption events. Value is of type AVAudioSessionInterruptionReason.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionreasonkey?language=objc)
    pub static AVAudioSessionInterruptionReasonKey: Option<&'static NSString>;
}

extern "C" {
    /// Only present in begin interruption events, where the interruption is a direct result of the
    /// application being suspended by the operating sytem. Value is a boolean NSNumber, where a true
    /// value indicates that the interruption is the result of the application being suspended, rather
    /// than being interrupted by another audio session.
    ///
    /// Starting in iOS 10, the system will deactivate the audio session of most apps in response to the
    /// app process being suspended. When the app starts running again, it will receive the notification
    /// that its session has been deactivated by the system. Note that the notification is necessarily
    /// delayed in time, due to the fact that the application was suspended at the time the session was
    /// deactivated by the system and the notification can only be delivered once the app is running
    /// again.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionwassuspendedkey?language=objc)
    #[deprecated = "No longer supported - see AVAudioSessionInterruptionReasonKey"]
    pub static AVAudioSessionInterruptionWasSuspendedKey: Option<&'static NSString>;
}

extern "C" {
    /// keys for AVAudioSessionRouteChangeNotification
    /// value is an NSNumber representing an AVAudioSessionRouteChangeReason
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionroutechangereasonkey?language=objc)
    pub static AVAudioSessionRouteChangeReasonKey: Option<&'static NSString>;
}

extern "C" {
    /// value is AVAudioSessionRouteDescription *
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionroutechangepreviousroutekey?language=objc)
    pub static AVAudioSessionRouteChangePreviousRouteKey: Option<&'static NSString>;
}

extern "C" {
    /// keys for AVAudioSessionSilenceSecondaryAudioHintNotification
    /// value is an NSNumber representing an AVAudioSessionSilenceSecondaryAudioHintType
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionsilencesecondaryaudiohinttypekey?language=objc)
    pub static AVAudioSessionSilenceSecondaryAudioHintTypeKey: Option<&'static NSString>;
}

extern "C" {
    /// keys for AVAudioSessionRenderingModeChangeNotification
    /// Contains a payload of NSInteger representing the new resolved rendering mode
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionrenderingmodenewrenderingmodekey?language=objc)
    pub static AVAudioSessionRenderingModeNewRenderingModeKey: Option<&'static NSString>;
}

extern "C" {
    /// Keys for AVAudioSessionMicrophoneInjectionCapabilitiesChangeNotification
    ///
    /// Indicates if microphone injection is available.
    /// Value is an NSNumber whose boolean value indicates if microphone injection is available.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmicrophoneinjectionisavailablekey?language=objc)
    pub static AVAudioSessionMicrophoneInjectionIsAvailableKey: Option<&'static NSString>;
}

extern "C" {
    /// Notification sent to registered listeners when there are changes in ``availableInputs``.
    ///
    /// There is no payload (userInfo dictionary) associated with the ``AVAudioSessionAvailableInputsChangeNotification`` notification.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionavailableinputschangenotification?language=objc)
    pub static AVAudioSessionAvailableInputsChangeNotification: Option<&'static NSNotificationName>;
}

/// For use with activateWithOptions:completionHandler:
///
/// Reserved for future use. Added in watchOS 5.0.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionactivationoptions?language=objc)
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionActivationOptions(pub NSUInteger);
bitflags::bitflags! {
    impl AVAudioSessionActivationOptions: NSUInteger {
        #[doc(alias = "AVAudioSessionActivationOptionNone")]
        const None = 0;
    }
}

unsafe impl Encode for AVAudioSessionActivationOptions {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionActivationOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// For use with overrideOutputAudioPort:error:
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionportoverride?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionPortOverride(pub NSUInteger);
impl AVAudioSessionPortOverride {
    /// No override.  Return audio routing to the default state for the current audio category.
    #[doc(alias = "AVAudioSessionPortOverrideNone")]
    pub const None: Self = Self(0);
    /// Route audio output to speaker.  Use this override with AVAudioSessionCategoryPlayAndRecord,
    /// which by default routes the output to the receiver.
    #[doc(alias = "AVAudioSessionPortOverrideSpeaker")]
    pub const Speaker: Self = Self(0x73706b72);
}

unsafe impl Encode for AVAudioSessionPortOverride {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionPortOverride {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values for AVAudioSessionRouteChangeReasonKey in AVAudioSessionRouteChangeNotification's
/// userInfo dictionary
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionroutechangereason?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionRouteChangeReason(pub NSUInteger);
impl AVAudioSessionRouteChangeReason {
    /// The reason is unknown.
    #[doc(alias = "AVAudioSessionRouteChangeReasonUnknown")]
    pub const Unknown: Self = Self(0);
    /// A new device became available (e.g. headphones have been plugged in).
    #[doc(alias = "AVAudioSessionRouteChangeReasonNewDeviceAvailable")]
    pub const NewDeviceAvailable: Self = Self(1);
    /// The old device became unavailable (e.g. headphones have been unplugged).
    #[doc(alias = "AVAudioSessionRouteChangeReasonOldDeviceUnavailable")]
    pub const OldDeviceUnavailable: Self = Self(2);
    /// The audio category has changed (e.g. AVAudioSessionCategoryPlayback has been changed to
    /// AVAudioSessionCategoryPlayAndRecord).
    #[doc(alias = "AVAudioSessionRouteChangeReasonCategoryChange")]
    pub const CategoryChange: Self = Self(3);
    /// The route has been overridden (e.g. category is AVAudioSessionCategoryPlayAndRecord and
    /// the output has been changed from the receiver, which is the default, to the speaker).
    #[doc(alias = "AVAudioSessionRouteChangeReasonOverride")]
    pub const Override: Self = Self(4);
    /// The device woke from sleep.
    #[doc(alias = "AVAudioSessionRouteChangeReasonWakeFromSleep")]
    pub const WakeFromSleep: Self = Self(6);
    /// Returned when there is no route for the current category (for instance, the category is
    /// AVAudioSessionCategoryRecord but no input device is available).
    #[doc(alias = "AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory")]
    pub const NoSuitableRouteForCategory: Self = Self(7);
    /// Indicates that the set of input and/our output ports has not changed, but some aspect of
    /// their configuration has changed.  For example, a port's selected data source has changed.
    /// (Introduced in iOS 7.0, watchOS 2.0, tvOS 9.0).
    #[doc(alias = "AVAudioSessionRouteChangeReasonRouteConfigurationChange")]
    pub const RouteConfigurationChange: Self = Self(8);
}

unsafe impl Encode for AVAudioSessionRouteChangeReason {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionRouteChangeReason {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Customization of various aspects of a category's behavior.
/// Use with ``AVAudioSession/setCategory:mode:options:error:``.
///
/// Applications must be prepared for changing category options to fail as behavior may change
/// in future releases. If an application changes its category, it should reassert the options,
/// since they are not sticky across category changes. Introduced in iOS 6.0 / watchOS 2.0 /
/// tvOS 9.0.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioncategoryoptions?language=objc)
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionCategoryOptions(pub NSUInteger);
bitflags::bitflags! {
    impl AVAudioSessionCategoryOptions: NSUInteger {
/// Controls whether other active audio apps will be interrupted or mixed with when your app's
/// audio session goes active. Details depend on the category.
///
/// - ``AVAudioSessionCategoryPlayAndRecord`` or ``AVAudioSessionCategoryMultiRoute``:
/// MixWithOthers defaults to false, but can be set to true, allowing other applications to
/// play in the background while your app has both audio input and output enabled.
///
/// - ``AVAudioSessionCategoryPlayback``:
/// MixWithOthers defaults to false, but can be set to true, allowing other applications to
/// play in the background. Your app will still be able to play regardless of the setting
/// of the ringer switch.
///
/// - Other categories:
/// MixWithOthers defaults to false and cannot be changed.
///
/// MixWithOthers is only valid with ``AVAudioSessionCategoryPlayAndRecord``,
/// ``AVAudioSessionCategoryPlayback``, and ``AVAudioSessionCategoryMultiRoute``.
        #[doc(alias = "AVAudioSessionCategoryOptionMixWithOthers")]
        const MixWithOthers = 0x1;
/// Controls whether or not other active audio apps will be ducked when when your app's audio
/// session goes active. An example of this is a workout app, which provides periodic updates to
/// the user. It reduces the volume of any music currently being played while it provides its
/// status.
///
/// Defaults to off. Note that the other audio will be ducked for as long as the current session
/// is active. You will need to deactivate your audio session when you want to restore full
/// volume playback (un-duck) other sessions.
///
/// Setting this option will also make your session mixable with others
/// (``AVAudioSessionCategoryOptionMixWithOthers`` will be set).
///
/// DuckOthers is only valid with ``AVAudioSessionCategoryAmbient``,
/// ``AVAudioSessionCategoryPlayAndRecord``, ``AVAudioSessionCategoryPlayback``, and
/// ``AVAudioSessionCategoryMultiRoute``.
        #[doc(alias = "AVAudioSessionCategoryOptionDuckOthers")]
        const DuckOthers = 0x2;
/// Deprecated - please see ``AVAudioSessionCategoryOptionAllowBluetoothHFP``
        #[doc(alias = "AVAudioSessionCategoryOptionAllowBluetooth")]
#[deprecated]
        const AllowBluetooth = 0x4;
/// Allows an application to change the default behavior of some audio session categories with
/// regard to whether Bluetooth Hands-Free Profile (HFP) devices are available for routing. The
/// exact behavior depends on the category.
///
/// - ``AVAudioSessionCategoryPlayAndRecord``:
/// AllowBluetoothHFP defaults to false, but can be set to true, allowing a paired bluetooth
/// HFP device to appear as an available route for input, while playing through the
/// category-appropriate output.
///
/// - ``AVAudioSessionCategoryRecord``:
/// AllowBluetoothHFP defaults to false, but can be set to true, allowing a paired Bluetooth
/// HFP device to appear as an available route for input.
///
/// - Other categories:
/// AllowBluetoothHFP defaults to false and cannot be changed. Enabling Bluetooth for input in
/// these categories is not allowed.
        #[doc(alias = "AVAudioSessionCategoryOptionAllowBluetoothHFP")]
        const AllowBluetoothHFP = 0x4;
/// Allows an application to change the default behavior of some audio session categories with
/// regard to the audio route. The exact behavior depends on the category.
///
/// - ``AVAudioSessionCategoryPlayAndRecord``:
/// DefaultToSpeaker will default to false, but can be set to true, routing to Speaker
/// (instead of Receiver) when no other audio route is connected.
///
/// - Other categories:
/// DefaultToSpeaker is always false and cannot be changed.
        #[doc(alias = "AVAudioSessionCategoryOptionDefaultToSpeaker")]
        const DefaultToSpeaker = 0x8;
/// When a session with InterruptSpokenAudioAndMixWithOthers set goes active, then if there is
/// another playing app whose session mode is ``AVAudioSessionModeSpokenAudio`` (for podcast
/// playback in the background, for example), then the spoken-audio session will be
/// interrupted. A good use of this is for a navigation app that provides prompts to its user:
/// it pauses any spoken audio currently being played while it plays the prompt.
///
/// InterruptSpokenAudioAndMixWithOthers defaults to off. Note that the other app's audio will
/// be paused for as long as the current session is active. You will need to deactivate your
/// audio session to allow the other session to resume playback. Setting this option will also
/// make your category mixable with others (``AVAudioSessionCategoryOptionMixWithOthers``
/// will be set). If you want other non-spoken audio apps to duck their audio when your app's session
/// goes active, also set ``AVAudioSessionCategoryOptionDuckOthers``.
///
/// Only valid with ``AVAudioSessionCategoryPlayAndRecord``,
/// ``AVAudioSessionCategoryPlayback``, and ``AVAudioSessionCategoryMultiRoute``.
        #[doc(alias = "AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers")]
        const InterruptSpokenAudioAndMixWithOthers = 0x11;
/// Allows an application to change the default behavior of some audio session categories with
/// regard to whether Bluetooth Advanced Audio Distribution Profile (A2DP) devices are
/// available for routing. The exact behavior depends on the category.
///
/// - ``AVAudioSessionCategoryPlayAndRecord``:
/// AllowBluetoothA2DP defaults to false, but can be set to true, allowing a paired
/// Bluetooth A2DP device to appear as an available route for output, while recording
/// through the category-appropriate input.
///
/// - ``AVAudioSessionCategoryMultiRoute`` and ``AVAudioSessionCategoryRecord``:
/// AllowBluetoothA2DP is false, and cannot be set to true.
///
/// - Other categories:
/// AllowBluetoothA2DP is always implicitly true and cannot be changed; Bluetooth A2DP ports
/// are always supported in output-only categories.
///
/// Setting both ``AVAudioSessionCategoryOptionAllowBluetoothHFP``
/// and ``AVAudioSessionCategoryOptionAllowBluetoothA2DP`` is
/// allowed. In cases where a single Bluetooth device supports both HFP and A2DP, the HFP
/// ports will be given a higher priority for routing. For HFP and A2DP ports on separate
/// hardware devices, the last-in wins rule applies.
        #[doc(alias = "AVAudioSessionCategoryOptionAllowBluetoothA2DP")]
        const AllowBluetoothA2DP = 0x20;
/// Allows an application to change the default behavior of some audio session categories
/// with regard to showing AirPlay devices as available routes. This option applies to
/// various categories in the same way as ``AVAudioSessionCategoryOptionAllowBluetoothA2DP``;
/// see above for details. Only valid with ``AVAudioSessionCategoryPlayAndRecord``.
        #[doc(alias = "AVAudioSessionCategoryOptionAllowAirPlay")]
        const AllowAirPlay = 0x40;
/// Some devices include a privacy feature that mutes the built-in microphone at a hardware level
/// under certain conditions e.g. when the Smart Folio of an iPad is closed. The default behavior is
/// to interrupt the session using the built-in microphone when that microphone is muted in hardware.
/// This option allows an application to opt out of the default behavior if it is using a category that
/// supports both input and output, such as ``AVAudioSessionCategoryPlayAndRecord``, and wants to
/// allow its session to stay activated even when the microphone has been muted. The result would be
/// that playback continues as normal, and microphone sample buffers would continue to be produced
/// but all microphone samples would have a value of zero.
///
/// This may be useful if an application knows that it wants to allow playback to continue and
/// recording/monitoring a muted microphone will not lead to a poor user experience. Attempting to use
/// this option with a session category that doesn't support the use of audio input will result in an error.
///
/// - Note Under the default policy, a session will be interrupted if it is running input at the time when
/// the microphone is muted in hardware. Similarly, attempting to start input when the microphone is
/// muted will fail.
/// - Note This option has no relation to the recordPermission property, which indicates whether or
/// not the user has granted permission to use microphone input.
        #[doc(alias = "AVAudioSessionCategoryOptionOverrideMutedMicrophoneInterruption")]
        const OverrideMutedMicrophoneInterruption = 0x80;
/// When this option is specified with a category that supports both input and output, the session
/// will enable full-bandwidth audio in both input
/// &
/// output directions, if the Bluetooth route supports
/// it (e.g. certain AirPods models). It is currently compatible only with mode ``AVAudioSessionModeDefault``.
///
/// - Support for this can be queried on input ports via the BluetoothMicrophone interface on a port,
/// via its member `highQualityRecording.isSupported`.
///
/// - Active sessions can see if full-bandwidth Bluetooth audio was successfully enabled by querying
/// the BluetoothMicrophone interface of the input port of the current route for:
/// `highQualityRecording.isEnabled`.
///
/// - When this option is provided alone, it will be enabled if the route supports it, otherwise the option
/// will be ignored. This option may be combined with ``AVAudioSessionCategoryOptionAllowBluetoothHFP``,
/// in which case HFP will be used as a fallback if the route does not support this
/// ``AVAudioSessionCategoryOptionBluetoothHighQualityRecording`` option.
///
/// - Note This option may increase input latency when enabled and is therefore not recommended for
/// real-time communication usage.
/// - Note Apps using ``AVAudioSessionCategoryOptionBluetoothHighQualityRecording``
/// may want to consider setting ``AVAudioSession/setPrefersNoInterruptionsFromSystemAlerts:error:``
/// while recording, to avoid the recording session being interrupted by an incoming call ringtone.
        #[doc(alias = "AVAudioSessionCategoryOptionBluetoothHighQualityRecording")]
        const BluetoothHighQualityRecording = 1<<19;
    }
}

unsafe impl Encode for AVAudioSessionCategoryOptions {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionCategoryOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values for AVAudioSessionInterruptionTypeKey in AVAudioSessionInterruptionNotification's
/// userInfo dictionary.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptiontype?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionInterruptionType(pub NSUInteger);
impl AVAudioSessionInterruptionType {
    /// the system has interrupted your audio session
    #[doc(alias = "AVAudioSessionInterruptionTypeBegan")]
    pub const Began: Self = Self(1);
    /// the interruption has ended
    #[doc(alias = "AVAudioSessionInterruptionTypeEnded")]
    pub const Ended: Self = Self(0);
}

unsafe impl Encode for AVAudioSessionInterruptionType {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionInterruptionType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values for AVAudioSessionInterruptionOptionKey in AVAudioSessionInterruptionNotification's
/// userInfo dictionary.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionoptions?language=objc)
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionInterruptionOptions(pub NSUInteger);
bitflags::bitflags! {
    impl AVAudioSessionInterruptionOptions: NSUInteger {
/// Indicates that you should resume playback now that the interruption has ended.
        #[doc(alias = "AVAudioSessionInterruptionOptionShouldResume")]
        const ShouldResume = 1;
    }
}

unsafe impl Encode for AVAudioSessionInterruptionOptions {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionInterruptionOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values for AVAudioSessionInterruptionReasonKey in AVAudioSessionInterruptionNotification's userInfo dictionary.
///
///
/// The audio session was interrupted because another session was activated.
///
///
/// The audio session was interrupted due to the app being suspended by the operating sytem.
/// Deprecated. Interruption notifications with reason 'wasSuspended' not present from iOS 16 onwards.
///
///
/// The audio session was interrupted due to the built-in mic being muted e.g. due to an iPad's Smart Folio being closed.
///
///
/// The audio session was interrupted due to route getting disconnected.
///
///
/// The audio session was interrupted due to device being doffed or locked.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioninterruptionreason?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionInterruptionReason(pub NSUInteger);
impl AVAudioSessionInterruptionReason {
    #[doc(alias = "AVAudioSessionInterruptionReasonDefault")]
    pub const Default: Self = Self(0);
    #[doc(alias = "AVAudioSessionInterruptionReasonAppWasSuspended")]
    #[deprecated = "wasSuspended reason no longer present"]
    pub const AppWasSuspended: Self = Self(1);
    #[doc(alias = "AVAudioSessionInterruptionReasonBuiltInMicMuted")]
    pub const BuiltInMicMuted: Self = Self(2);
    /// The audio session was interrupted because route was disconnected.
    #[doc(alias = "AVAudioSessionInterruptionReasonRouteDisconnected")]
    pub const RouteDisconnected: Self = Self(4);
}

unsafe impl Encode for AVAudioSessionInterruptionReason {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionInterruptionReason {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// options for use when calling setActive:withOptions:error:
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionsetactiveoptions?language=objc)
// NS_OPTIONS
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionSetActiveOptions(pub NSUInteger);
bitflags::bitflags! {
    impl AVAudioSessionSetActiveOptions: NSUInteger {
/// Notify an interrupted app that the interruption has ended and it may resume playback. Only
/// valid on session deactivation.
        #[doc(alias = "AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation")]
        const NotifyOthersOnDeactivation = 1;
    }
}

unsafe impl Encode for AVAudioSessionSetActiveOptions {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionSetActiveOptions {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values for AVAudioSessionSilenceSecondaryAudioHintTypeKey in
/// AVAudioSessionSilenceSecondaryAudioHintNotification's userInfo dictionary, to indicate whether
/// optional secondary audio muting should begin or end.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionsilencesecondaryaudiohinttype?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionSilenceSecondaryAudioHintType(pub NSUInteger);
impl AVAudioSessionSilenceSecondaryAudioHintType {
    /// Another application's primary audio has started.
    #[doc(alias = "AVAudioSessionSilenceSecondaryAudioHintTypeBegin")]
    pub const Begin: Self = Self(1);
    /// Another application's primary audio has stopped.
    #[doc(alias = "AVAudioSessionSilenceSecondaryAudioHintTypeEnd")]
    pub const End: Self = Self(0);
}

unsafe impl Encode for AVAudioSessionSilenceSecondaryAudioHintType {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionSilenceSecondaryAudioHintType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Values to be used by setAggregatedIOPreference:error: method.
///
/// Starting in iOS 10, applications that use AVCaptureSession on iPads and iPhones that
/// support taking Live Photos, will have non-aggregated audio I/O unless the app opts out by
/// setting its AVAudioSessionIOType to Aggregated. Non-aggregated audio I/O means that separate
/// threads will be used to service audio I/O for input and output directions.
///
/// Note that in cases where the I/O is not aggregated, the sample rate and IO buffer duration
/// properties will map to the output audio device. In this scenario, the input and
/// output audio hardware may be running at different sample rates and with different IO buffer
/// durations. If your app requires input and output audio to be presented in the same realtime
/// I/O callback, or requires that input and output audio have the same sample rate or IO buffer
/// duration, or if your app requires the ability to set a preferred sample rate or IO buffer duration
/// for audio input, set the AVAudioSessionIOType to Aggregated.
///
/// Apps that don't use AVCaptureSession and use AVAudioSessionCategoryPlayAndRecord will continue
/// to have aggregated audio I/O, as in previous versions of iOS.
///
///
/// The default value.  If your app does not use AVCaptureSession or does not have any specific
/// requirement for aggregating input and output audio in the same realtime I/O callback, use this
/// value. Note that if your app does not use AVCaptureSession, it will get aggregated I/O when using
/// AVAudioSessionCategoryPlayAndRecord.
///
/// If your app does utilize AVCaptureSession, use of this value will allow AVCaptureSession to
/// start recording without glitching already running output audio and will allow the system to
/// utilize power-saving optimizations.
///
///
/// Use this value if your session uses AVAudioSessionCategoryPlayAndRecord and requires input and
/// output audio to be presented in the same realtime I/O callback. For example, if your app will be using
/// a RemoteIO with both input and output enabled.
///
/// Note that your session's preference to use aggregated IO will not be honored if it specifies
/// AVAudioSessionCategoryOptionMixWithOthers AND another app's audio session was already active
/// with non-mixable, non-aggregated input/output.
///
/// Added in iOS 10.0. Not applicable on watchos, tvos, macos.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessioniotype?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionIOType(pub NSUInteger);
impl AVAudioSessionIOType {
    #[doc(alias = "AVAudioSessionIOTypeNotSpecified")]
    pub const NotSpecified: Self = Self(0);
    #[doc(alias = "AVAudioSessionIOTypeAggregated")]
    pub const Aggregated: Self = Self(1);
}

unsafe impl Encode for AVAudioSessionIOType {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionIOType {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Starting in iOS 11, tvOS 11, and watchOS 5, the route sharing policy allows a session
/// to specify that its output audio should be routed somewhere other than the default system output,
/// when appropriate alternative routes are available.
///
/// Follow normal rules for routing audio output.
///
/// Route output to the shared long-form audio output. A session whose primary use case is as a
/// music or podcast player may use this value to play to the same output as the built-in Music (iOS),
/// Podcasts, or iTunes (macOS) applications. Typically applications that use this policy will also
/// want sign up for remote control events as documented in Event Handling Guide for UIKit Apps
/// and will want to utilize MediaPlayer frameworks MPNowPlayingInfoCenter class. All applications
/// on the system that use the long-form audio route sharing policy will have their audio routed to the
/// same location.
/// Apps running on watchOS using this policy will also be able to play audio in the background,
/// as long as an eligible audio route can be activated. Apps running on watchOS using this policy
/// must use -activateWithOptions:completionHandler: instead of -setActive:withOptions:error: in
/// order to ensure that the user will be given the opportunity to pick an appropriate audio route
/// in cases where the system is unable to automatically pick the route.
///
/// Deprecated. Replaced by AVAudioSessionRouteSharingPolicyLongFormAudio.
///
/// Applications should not attempt to set this value directly. On iOS, this value will be set by
/// the system in cases where route picker UI is used to direct video to a wireless route.
///
/// Route output to the shared long-form video output. A session whose primary use case is as a
/// movie or other long-form video content player may use this value to play to the same output as
/// other long-form video content applications such as the built-in TV (iOS) application. Applications
/// that use this policy will also want to also set the AVInitialRouteSharingPolicy key
/// in their Info.plist to "LongFormVideo". All applications on the system that use the long-form video
/// route sharing policy will have their audio and video routed to the same location (e.g. AppleTV when
/// an AirPlay route is selected). Video content not using this route sharing policy will remain local
/// to the playback device even when long form video content is being routed to AirPlay.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionroutesharingpolicy?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionRouteSharingPolicy(pub NSUInteger);
impl AVAudioSessionRouteSharingPolicy {
    #[doc(alias = "AVAudioSessionRouteSharingPolicyDefault")]
    pub const Default: Self = Self(0);
    #[doc(alias = "AVAudioSessionRouteSharingPolicyLongFormAudio")]
    pub const LongFormAudio: Self = Self(1);
    #[doc(alias = "AVAudioSessionRouteSharingPolicyLongForm")]
    #[deprecated]
    pub const LongForm: Self = Self(AVAudioSessionRouteSharingPolicy::LongFormAudio.0);
    #[doc(alias = "AVAudioSessionRouteSharingPolicyIndependent")]
    pub const Independent: Self = Self(2);
    #[doc(alias = "AVAudioSessionRouteSharingPolicyLongFormVideo")]
    pub const LongFormVideo: Self = Self(3);
}

unsafe impl Encode for AVAudioSessionRouteSharingPolicy {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionRouteSharingPolicy {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// The prompt style is a hint to sessions that use AVAudioSessionModeVoicePrompt to modify the type of
/// prompt they play in response to other audio activity on the system, such as Siri or phone calls.
/// Sessions that issue voice prompts are encouraged to pay attention to changes in the prompt style and
/// modify their prompts in response. Apple encourages the use of non-verbal prompts when the Short
/// style is requested.
///
/// Indicates that another session is actively using microphone input and would be negatively impacted
/// by having prompts play at that time. For example if Siri is recognizing speech, having navigation or
/// exercise prompts play, could interfere with its ability to accurately recognize the users speech.
/// Client sessions should refrain from playing any prompts while the prompt style is None.
///
/// Indicates one of three states: Siri is active but not recording, voicemail playback is active, or
/// voice call is active. Short, non-verbal versions of prompts should be used.
///
/// Indicates that normal (long, verbal) versions of prompts may be used.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionpromptstyle?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionPromptStyle(pub NSUInteger);
impl AVAudioSessionPromptStyle {
    #[doc(alias = "AVAudioSessionPromptStyleNone")]
    pub const None: Self = Self(0x6e6f6e65);
    #[doc(alias = "AVAudioSessionPromptStyleShort")]
    pub const Short: Self = Self(0x73687274);
    #[doc(alias = "AVAudioSessionPromptStyleNormal")]
    pub const Normal: Self = Self(0x6e726d6c);
}

unsafe impl Encode for AVAudioSessionPromptStyle {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionPromptStyle {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Constants indicating stereo input audio orientation, for use with built-in mic input data sources with a stereo polar pattern selected.
///
///
/// Indicates that audio capture orientation is not applicable (on mono capture, for instance).
///
/// Indicates that audio capture should be oriented vertically, Lightning connector on the bottom.
///
/// Indicates that audio capture should be oriented vertically, Lightning connector on the top.
///
/// Indicates that audio capture should be oriented horizontally, Lightning connector on the right.
///
/// Indicates that audio capture should be oriented horizontally, Lightning connector on the left.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiostereoorientation?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioStereoOrientation(pub NSInteger);
impl AVAudioStereoOrientation {
    #[doc(alias = "AVAudioStereoOrientationNone")]
    pub const None: Self = Self(0);
    #[doc(alias = "AVAudioStereoOrientationPortrait")]
    pub const Portrait: Self = Self(1);
    #[doc(alias = "AVAudioStereoOrientationPortraitUpsideDown")]
    pub const PortraitUpsideDown: Self = Self(2);
    #[doc(alias = "AVAudioStereoOrientationLandscapeRight")]
    pub const LandscapeRight: Self = Self(3);
    #[doc(alias = "AVAudioStereoOrientationLandscapeLeft")]
    pub const LandscapeLeft: Self = Self(4);
}

unsafe impl Encode for AVAudioStereoOrientation {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioStereoOrientation {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// These are the values returned by recordPermission.
///
/// The user has not yet been asked for permission.
///
/// The user has been asked and has denied permission.
///
/// The user has been asked and has granted permission.
///
/// Introduced: ios(8.0), watchos(4.0)
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionrecordpermission?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionRecordPermission(pub NSUInteger);
impl AVAudioSessionRecordPermission {
    #[doc(alias = "AVAudioSessionRecordPermissionUndetermined")]
    #[deprecated]
    pub const Undetermined: Self = Self(0x756e6474);
    #[doc(alias = "AVAudioSessionRecordPermissionDenied")]
    #[deprecated]
    pub const Denied: Self = Self(0x64656e79);
    #[doc(alias = "AVAudioSessionRecordPermissionGranted")]
    #[deprecated]
    pub const Granted: Self = Self(0x67726e74);
}

unsafe impl Encode for AVAudioSessionRecordPermission {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionRecordPermission {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionrenderingmode?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionRenderingMode(pub NSInteger);
impl AVAudioSessionRenderingMode {
    /// Default Mode when no asset is loaded or playing
    #[doc(alias = "AVAudioSessionRenderingModeNotApplicable")]
    pub const NotApplicable: Self = Self(0);
    /// Default mode for non multi-channel cases
    #[doc(alias = "AVAudioSessionRenderingModeMonoStereo")]
    pub const MonoStereo: Self = Self(1);
    /// Default mode for multi-channel cases that do not fall into the modes below
    #[doc(alias = "AVAudioSessionRenderingModeSurround")]
    pub const Surround: Self = Self(2);
    /// Fallback mode if provided content is Dolby variant but hardware capabilities don't support it
    #[doc(alias = "AVAudioSessionRenderingModeSpatialAudio")]
    pub const SpatialAudio: Self = Self(3);
    /// Dolby Audio mode
    #[doc(alias = "AVAudioSessionRenderingModeDolbyAudio")]
    pub const DolbyAudio: Self = Self(4);
    /// Dolby Atmos mode
    #[doc(alias = "AVAudioSessionRenderingModeDolbyAtmos")]
    pub const DolbyAtmos: Self = Self(5);
}

unsafe impl Encode for AVAudioSessionRenderingMode {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionRenderingMode {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Various modes to inject audio coming from a session to another apps input stream
///
/// Applications can state their intent to mix locally generated audio, which should consist primarily of
/// synthesized speech, to another app's input stream. This feature is intended to be used by accessibility apps
/// implementing augmentative and alternative communication systems that enable users with disabilities to
/// communicate with synthesized speech. When input is muted, microphone injection will also be muted.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfaudio/avaudiosessionmicrophoneinjectionmode?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVAudioSessionMicrophoneInjectionMode(pub NSInteger);
impl AVAudioSessionMicrophoneInjectionMode {
    /// Default state, microphone injection is not preferred
    #[doc(alias = "AVAudioSessionMicrophoneInjectionModeNone")]
    pub const None: Self = Self(0);
    /// Inject Spoken Audio, like synthesized speech, with microphone audio
    #[doc(alias = "AVAudioSessionMicrophoneInjectionModeSpokenAudio")]
    pub const SpokenAudio: Self = Self(1);
}

unsafe impl Encode for AVAudioSessionMicrophoneInjectionMode {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

unsafe impl RefEncode for AVAudioSessionMicrophoneInjectionMode {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}
