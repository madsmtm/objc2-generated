//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
use objc2::__framework_prelude::*;
#[cfg(feature = "objc2-core-foundation")]
use objc2_core_foundation::*;
use objc2_foundation::*;

use crate::*;

extern_class!(
    /// AVCaptureOutput is an abstract class that defines an interface for an output destination of an AVCaptureSession.
    ///
    ///
    /// AVCaptureOutput provides an abstract interface for connecting capture output destinations, such as files and video previews, to an AVCaptureSession.
    ///
    /// An AVCaptureOutput can have multiple connections represented by AVCaptureConnection objects, one for each stream of media that it receives from an AVCaptureInput. An AVCaptureOutput does not have any connections when it is first created. When an output is added to an AVCaptureSession, connections are created that map media data from that session's inputs to its outputs.
    ///
    /// Concrete AVCaptureOutput instances can be added to an AVCaptureSession using the -[AVCaptureSession addOutput:] and -[AVCaptureSession addOutputWithNoConnections:] methods.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/avfoundation/avcaptureoutput?language=objc)
    #[unsafe(super(NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    pub struct AVCaptureOutput;
);

extern_conformance!(
    unsafe impl NSObjectProtocol for AVCaptureOutput {}
);

impl AVCaptureOutput {
    extern_methods!(
        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;

        #[cfg(feature = "AVCaptureSession")]
        /// The connections that describe the flow of media data to the receiver from AVCaptureInputs.
        ///
        ///
        /// The value of this property is an NSArray of AVCaptureConnection objects, each describing the mapping between the receiver and the AVCaptureInputPorts of one or more AVCaptureInputs.
        #[unsafe(method(connections))]
        #[unsafe(method_family = none)]
        pub unsafe fn connections(&self) -> Retained<NSArray<AVCaptureConnection>>;

        #[cfg(all(feature = "AVCaptureSession", feature = "AVMediaFormat"))]
        /// Returns the first connection in the connections array with an inputPort of the specified mediaType.
        ///
        ///
        /// Parameter `mediaType`: An AVMediaType constant from AVMediaFormat.h, e.g. AVMediaTypeVideo.
        ///
        ///
        /// This convenience method returns the first AVCaptureConnection in the receiver's connections array that has an AVCaptureInputPort of the specified mediaType. If no connection with the specified mediaType is found, nil is returned.
        #[unsafe(method(connectionWithMediaType:))]
        #[unsafe(method_family = none)]
        pub unsafe fn connectionWithMediaType(
            &self,
            media_type: &AVMediaType,
        ) -> Option<Retained<AVCaptureConnection>>;

        #[cfg(all(feature = "AVCaptureSession", feature = "AVMetadataObject"))]
        /// Converts an AVMetadataObject's visual properties to the receiver's coordinates.
        ///
        ///
        /// Parameter `metadataObject`: An AVMetadataObject originating from the same AVCaptureInput as the receiver.
        ///
        /// Parameter `connection`: The receiver's connection whose AVCaptureInput matches that of the metadata object to be converted.
        ///
        /// Returns: An AVMetadataObject whose properties are in output coordinates.
        ///
        ///
        /// AVMetadataObject bounds may be expressed as a rect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. Face metadata objects likewise express yaw and roll angles with respect to an unrotated picture. -transformedMetadataObjectForMetadataObject:connection: converts the visual properties in the coordinate space of the supplied AVMetadataObject to the coordinate space of the receiver. The conversion takes orientation, mirroring, and scaling into consideration. If the provided metadata object originates from an input source other than the preview layer's, nil will be returned.
        ///
        /// If an AVCaptureVideoDataOutput instance's connection's videoOrientation or videoMirrored properties are set to non-default values, the output applies the desired mirroring and orientation by physically rotating and or flipping sample buffers as they pass through it. AVCaptureStillImageOutput, on the other hand, does not physically rotate its buffers. It attaches an appropriate kCGImagePropertyOrientation number to captured still image buffers (see ImageIO/CGImageProperties.h) indicating how the image should be displayed on playback. Likewise, AVCaptureMovieFileOutput does not physically apply orientation/mirroring to its sample buffers -- it uses a QuickTime track matrix to indicate how the buffers should be rotated and/or flipped on playback.
        ///
        /// transformedMetadataObjectForMetadataObject:connection: alters the visual properties of the provided metadata object to match the physical rotation / mirroring of the sample buffers provided by the receiver through the indicated connection. I.e., for video data output, adjusted metadata object coordinates are rotated/mirrored. For still image and movie file output, they are not.
        #[unsafe(method(transformedMetadataObjectForMetadataObject:connection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn transformedMetadataObjectForMetadataObject_connection(
            &self,
            metadata_object: &AVMetadataObject,
            connection: &AVCaptureConnection,
        ) -> Option<Retained<AVMetadataObject>>;

        #[cfg(feature = "objc2-core-foundation")]
        /// Converts a rectangle in the receiver's coordinate space to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the receiver.
        ///
        ///
        /// Parameter `rectInOutputCoordinates`: A CGRect in the receiver's coordinates.
        ///
        /// Returns: A CGRect in the coordinate space of the metadata output whose capture device is providing input to the receiver.
        ///
        ///
        /// AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of the receiver to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the receiver. The conversion takes orientation, mirroring, and scaling into consideration. See -transformedMetadataObjectForMetadataObject:connection: for a full discussion of how orientation and mirroring are applied to sample buffers passing through the output.
        #[unsafe(method(metadataOutputRectOfInterestForRect:))]
        #[unsafe(method_family = none)]
        pub unsafe fn metadataOutputRectOfInterestForRect(
            &self,
            rect_in_output_coordinates: CGRect,
        ) -> CGRect;

        #[cfg(feature = "objc2-core-foundation")]
        /// Converts a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the receiver to a rectangle in the receiver's coordinates.
        ///
        ///
        /// Parameter `rectInMetadataOutputCoordinates`: A CGRect in the coordinate space of the metadata output whose capture device is providing input to the receiver.
        ///
        /// Returns: A CGRect in the receiver's coordinates.
        ///
        ///
        /// AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the coordinate space of the receiver. The conversion takes orientation, mirroring, and scaling into consideration. See -transformedMetadataObjectForMetadataObject:connection: for a full discussion of how orientation and mirroring are applied to sample buffers passing through the output.
        #[unsafe(method(rectForMetadataOutputRectOfInterest:))]
        #[unsafe(method_family = none)]
        pub unsafe fn rectForMetadataOutputRectOfInterest(
            &self,
            rect_in_metadata_output_coordinates: CGRect,
        ) -> CGRect;

        /// A `BOOL` value that indicates whether the output supports deferred start.
        ///
        /// You can only set the ``deferredStartEnabled`` property value to `true` if the output supports deferred start.
        #[unsafe(method(isDeferredStartSupported))]
        #[unsafe(method_family = none)]
        pub unsafe fn isDeferredStartSupported(&self) -> bool;

        /// A `BOOL` value that indicates whether to defer starting this capture output.
        ///
        /// When this value is `true`, the session does not prepare the output's resources until some time after ``AVCaptureSession/startRunning`` returns. You can start the visual parts of your user interface (e.g. preview) prior to other parts (e.g. photo/movie capture, metadata output, etc..) to improve startup performance. Set this value to `false` for outputs that your app needs for startup, and `true` for the ones it does not need to start immediately. For example, an ``AVCaptureVideoDataOutput`` that you intend to use for displaying preview should set this value to `false`, so that the frames are available as soon as possible.
        ///
        /// By default, for apps that are linked on or after iOS 26, this property value is `true` for ``AVCapturePhotoOutput`` and ``AVCaptureFileOutput`` subclasses if supported, and `false` otherwise. When set to `true` for ``AVCapturePhotoOutput``, if you want to support multiple capture requests before running deferred start, set ``AVCapturePhotoOutput/responsiveCaptureEnabled`` to `true` on that output.
        ///
        /// If ``deferredStartSupported`` is `false`, setting this property value to `true` results in the system throwing an `NSInvalidArgumentException`.
        ///
        /// - Note: Set this value before calling ``AVCaptureSession/commitConfiguration`` as it requires a lengthy reconfiguration of the capture render pipeline.
        #[unsafe(method(isDeferredStartEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isDeferredStartEnabled(&self) -> bool;

        /// Setter for [`isDeferredStartEnabled`][Self::isDeferredStartEnabled].
        #[unsafe(method(setDeferredStartEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDeferredStartEnabled(&self, deferred_start_enabled: bool);
    );
}

/// Constants indicating the reason a capture data output dropped data.
///
///
/// No data was dropped.
///
/// Data was dropped because alwaysDiscardsLate{VideoFrames | DepthData} is YES and the client was still processing previous data when the current data needed to be delivered.
///
/// Data was dropped because its pool of buffers ran dry. This is usually indicative that the client is holding onto data objects too long.
///
/// Data was dropped because the device providing the data experienced a discontinuity, and an unknown number of data objects have been lost. This condition is typically caused by the system being too busy.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/avfoundation/avcaptureoutputdatadroppedreason?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AVCaptureOutputDataDroppedReason(pub NSInteger);
impl AVCaptureOutputDataDroppedReason {
    #[doc(alias = "AVCaptureOutputDataDroppedReasonNone")]
    pub const None: Self = Self(0);
    #[doc(alias = "AVCaptureOutputDataDroppedReasonLateData")]
    pub const LateData: Self = Self(1);
    #[doc(alias = "AVCaptureOutputDataDroppedReasonOutOfBuffers")]
    pub const OutOfBuffers: Self = Self(2);
    #[doc(alias = "AVCaptureOutputDataDroppedReasonDiscontinuity")]
    pub const Discontinuity: Self = Self(3);
}

unsafe impl Encode for AVCaptureOutputDataDroppedReason {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

unsafe impl RefEncode for AVCaptureOutputDataDroppedReason {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}
