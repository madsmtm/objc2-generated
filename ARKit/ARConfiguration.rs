//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
#[cfg(feature = "objc2")]
use objc2::__framework_prelude::*;
#[cfg(feature = "objc2-av-foundation")]
use objc2_av_foundation::*;
#[cfg(feature = "objc2-core-location")]
use objc2_core_location::*;
#[cfg(feature = "objc2-foundation")]
use objc2_foundation::*;

use crate::*;

/// Option set indicating semantic understanding types of the image frame.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arframesemantics?language=objc)
// NS_OPTIONS
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARFrameSemantics(pub NSUInteger);
#[cfg(feature = "objc2")]
bitflags::bitflags! {
    impl ARFrameSemantics: NSUInteger {
/// No semantic operation is run.
        #[doc(alias = "ARFrameSemanticNone")]
        const None = 0;
/// Person segmentation.
///
/// A pixel in the image frame that gets classified as person will have an intensity value equal to 'ARSegmentationClassPerson'.
///
/// See: -[ARFrame segmentationBuffer]
///
/// See: ARSegmentationClass
        #[doc(alias = "ARFrameSemanticPersonSegmentation")]
        const PersonSegmentation = 1<<0;
/// Person segmentation with depth.
///
/// A pixel in the image frame that gets classified as person will have an intensity value equal to 'ARSegmentationClassPerson'.
/// Additionally, every pixel in the image frame that gets classified as person will also have a depth value.
///
/// See: -[ARFrame estimatedDepthData]
///
/// See: -[ARFrame segmentationBuffer]
        #[doc(alias = "ARFrameSemanticPersonSegmentationWithDepth")]
        const PersonSegmentationWithDepth = (1<<1)|(1<<0);
/// Body detection.
///
/// Once activated an ARFrame will contain information about a detected body.
///
/// See: -[ARFrame detectedBody]
///
/// See: ARBody2D
        #[doc(alias = "ARFrameSemanticBodyDetection")]
        const BodyDetection = 1<<2;
/// Scene Depth.
///
/// Each capturedImage will have an associated scene depth data.
///
/// See: - [ARFrame sceneDepth]
        #[doc(alias = "ARFrameSemanticSceneDepth")]
        const SceneDepth = 1<<3;
/// Smoothed Scene Depth.
///
/// Each capturedImage will have an associated scene depth data that is temporally smoothed.
///
/// See: - [ARFrame smoothedSceneDepth]
        #[doc(alias = "ARFrameSemanticSmoothedSceneDepth")]
        const SmoothedSceneDepth = 1<<4;
    }
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARFrameSemantics {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARFrameSemantics {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Enum constants for indicating the world alignment.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arworldalignment?language=objc)
// NS_ENUM
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARWorldAlignment(pub NSInteger);
#[cfg(feature = "objc2")]
impl ARWorldAlignment {
    /// Aligns the world with gravity that is defined by vector (0, -1, 0).
    #[doc(alias = "ARWorldAlignmentGravity")]
    pub const Gravity: Self = Self(0);
    /// Aligns the world with gravity that is defined by the vector (0, -1, 0)
    /// and heading (w.r.t. True North) that is given by the vector (0, 0, -1).
    #[doc(alias = "ARWorldAlignmentGravityAndHeading")]
    pub const GravityAndHeading: Self = Self(1);
    /// Aligns the world with the cameraâ€™s orientation.
    #[doc(alias = "ARWorldAlignmentCamera")]
    pub const Camera: Self = Self(2);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARWorldAlignment {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARWorldAlignment {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Enum constants for indicating the mode of environment texturing to run.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arenvironmenttexturing?language=objc)
// NS_ENUM
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AREnvironmentTexturing(pub NSInteger);
#[cfg(feature = "objc2")]
impl AREnvironmentTexturing {
    /// No texture information is gathered.
    #[doc(alias = "AREnvironmentTexturingNone")]
    pub const None: Self = Self(0);
    /// Texture information is gathered for the environment.
    /// Environment textures will be generated for AREnvironmentProbes added to the session.
    #[doc(alias = "AREnvironmentTexturingManual")]
    pub const Manual: Self = Self(1);
    /// Texture information is gathered for the environment and probes automatically placed in the scene.
    #[doc(alias = "AREnvironmentTexturingAutomatic")]
    pub const Automatic: Self = Self(2);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for AREnvironmentTexturing {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for AREnvironmentTexturing {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Types of scene reconstruction.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arscenereconstruction?language=objc)
// NS_OPTIONS
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARSceneReconstruction(pub NSUInteger);
#[cfg(feature = "objc2")]
bitflags::bitflags! {
    impl ARSceneReconstruction: NSUInteger {
/// No scene reconstruction is run.
        #[doc(alias = "ARSceneReconstructionNone")]
        const None = 0;
/// Scene reconstruction generates a mesh of the world
        #[doc(alias = "ARSceneReconstructionMesh")]
        const Mesh = 1<<0;
/// Scene reconstruction generates a mesh of the world with classification for each face.
        #[doc(alias = "ARSceneReconstructionMeshWithClassification")]
        const MeshWithClassification = (1<<1)|(1<<0);
    }
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARSceneReconstruction {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARSceneReconstruction {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

#[cfg(feature = "objc2")]
extern_class!(
    /// An object to describe and configure the Augmented Reality techniques to be used in an ARSession.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arconfiguration?language=objc)
    #[unsafe(super(NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARConfiguration {
    extern_methods!(
        /// Determines whether this device supports the ARConfiguration.
        #[unsafe(method(isSupported))]
        #[unsafe(method_family = none)]
        pub unsafe fn isSupported() -> bool;

        #[cfg(all(feature = "ARVideoFormat", feature = "objc2-foundation"))]
        /// A list of supported video formats for this configuration and device.
        ///
        /// The first element in the list is the default format for session output.
        #[unsafe(method(supportedVideoFormats))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportedVideoFormats() -> Retained<NSArray<ARVideoFormat>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Video format of the session output.
        #[unsafe(method(videoFormat))]
        #[unsafe(method_family = none)]
        pub unsafe fn videoFormat(&self) -> Retained<ARVideoFormat>;

        #[cfg(feature = "ARVideoFormat")]
        /// Setter for [`videoFormat`][Self::videoFormat].
        #[unsafe(method(setVideoFormat:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setVideoFormat(&self, video_format: &ARVideoFormat);

        /// Determines how the coordinate system should be aligned with the world.
        ///
        /// The default is ARWorldAlignmentGravity.
        #[unsafe(method(worldAlignment))]
        #[unsafe(method_family = none)]
        pub unsafe fn worldAlignment(&self) -> ARWorldAlignment;

        /// Setter for [`worldAlignment`][Self::worldAlignment].
        #[unsafe(method(setWorldAlignment:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWorldAlignment(&self, world_alignment: ARWorldAlignment);

        /// Enable or disable light estimation.
        ///
        /// Enabled by default.
        #[unsafe(method(isLightEstimationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isLightEstimationEnabled(&self) -> bool;

        /// Setter for [`isLightEstimationEnabled`][Self::isLightEstimationEnabled].
        #[unsafe(method(setLightEstimationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setLightEstimationEnabled(&self, light_estimation_enabled: bool);

        /// Determines whether to capture and provide audio data.
        ///
        /// Disabled by default.
        #[unsafe(method(providesAudioData))]
        #[unsafe(method_family = none)]
        pub unsafe fn providesAudioData(&self) -> bool;

        /// Setter for [`providesAudioData`][Self::providesAudioData].
        #[unsafe(method(setProvidesAudioData:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setProvidesAudioData(&self, provides_audio_data: bool);

        /// The type of semantic understanding to provide with each frame.
        ///
        ///
        /// Use the `supportsFrameSemantics` class method to check if the configuration type you intend to run supports the set of frame semantics.
        /// For example, when running a session with a configuration of type ARWorldTrackingConfiguration one would need to use `+[ ARWorldTrackingConfiguration
        /// supportsFrameSemantics:]` to perform said check. An exception is thrown if the option is not supported. Defaults to ARFrameSemanticNone.
        ///
        /// See: ARFrameSemantics
        ///
        /// See: +[ARConfiguration supportsFrameSemantics:]
        #[unsafe(method(frameSemantics))]
        #[unsafe(method_family = none)]
        pub unsafe fn frameSemantics(&self) -> ARFrameSemantics;

        /// Setter for [`frameSemantics`][Self::frameSemantics].
        #[unsafe(method(setFrameSemantics:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setFrameSemantics(&self, frame_semantics: ARFrameSemantics);

        /// Determines whether the type of frame semantics is supported by the device and ARConfiguration class.
        ///
        ///
        /// Semantic frame understanding is not supported on all devices. Use the `supportsFrameSemantics` class method to check if the configuration
        /// type you intend to run supports the set of frame semantics. For example, when running a session with a configuration of type
        /// ARWorldTrackingConfiguration one would need to use
        /// `+[ ARWorldTrackingConfiguration supportsFrameSemantics:]` to perform said check.
        ///
        /// See: ARFrameSemantics
        #[unsafe(method(supportsFrameSemantics:))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsFrameSemantics(frame_semantics: ARFrameSemantics) -> bool;

        #[cfg(feature = "objc2-av-foundation")]
        /// Returns a pointer to the capture device of the camera that's used for rendering, so developers can adjust capture settings.
        ///
        /// May return nil if it is not recommended to modify capture settings, for example if the primary camera is used for tracking.
        #[unsafe(method(configurableCaptureDeviceForPrimaryCamera))]
        #[unsafe(method_family = none)]
        pub unsafe fn configurableCaptureDeviceForPrimaryCamera(
        ) -> Option<Retained<AVCaptureDevice>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Returns a video format using a 4K resolution from the list of supported video formats.
        ///
        /// May return nil if 4K is not supported for this configuration or device.
        #[unsafe(method(recommendedVideoFormatFor4KResolution))]
        #[unsafe(method_family = none)]
        pub unsafe fn recommendedVideoFormatFor4KResolution() -> Option<Retained<ARVideoFormat>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Returns a recommended video format that supports capturing high resolution frames with a significantly higher resolution than the streaming camera
        /// resolution.
        ///
        /// Using this format may consume more power. Other video formats may support capturing high resolution frames as well, albeit at a lower
        /// quality or resolution.
        ///
        /// See: [ARSession captureHighResolutionFrameWithCompletion:]
        #[unsafe(method(recommendedVideoFormatForHighResolutionFrameCapturing))]
        #[unsafe(method_family = none)]
        pub unsafe fn recommendedVideoFormatForHighResolutionFrameCapturing(
        ) -> Option<Retained<ARVideoFormat>>;

        /// Whether HDR capturing is allowed if the current video format supports it. Defaults to
        /// `NO.`
        #[unsafe(method(videoHDRAllowed))]
        #[unsafe(method_family = none)]
        pub unsafe fn videoHDRAllowed(&self) -> bool;

        /// Setter for [`videoHDRAllowed`][Self::videoHDRAllowed].
        #[unsafe(method(setVideoHDRAllowed:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setVideoHDRAllowed(&self, video_hdr_allowed: bool);

        /// Unavailable
        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running world tracking.
    ///
    ///
    /// World tracking provides 6 degrees of freedom tracking of the device.
    /// By finding feature points in the scene, world tracking enables performing hit-tests against the frame.
    /// Tracking can no longer be resumed once the session is paused.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARWorldTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARWorldTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARWorldTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARWorldTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARWorldTrackingConfiguration {
    extern_methods!(
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[unsafe(method(isAutoFocusEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[unsafe(method(setAutoFocusEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[unsafe(method(environmentTexturing))]
        #[unsafe(method_family = none)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[unsafe(method(setEnvironmentTexturing:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[unsafe(method(wantsHDREnvironmentTextures))]
        #[unsafe(method_family = none)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[unsafe(method(setWantsHDREnvironmentTextures:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[unsafe(method(planeDetection))]
        #[unsafe(method_family = none)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[unsafe(method(setPlaneDetection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[unsafe(method(initialWorldMap))]
        #[unsafe(method_family = none)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[unsafe(method(setInitialWorldMap:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[unsafe(method(detectionImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[unsafe(method(setDetectionImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDetectionImages(&self, detection_images: Option<&NSSet<ARReferenceImage>>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[unsafe(method(automaticImageScaleEstimationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[unsafe(method(setAutomaticImageScaleEstimationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[unsafe(method(maximumNumberOfTrackedImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[unsafe(method(setMaximumNumberOfTrackedImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Objects to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified objects. When an object is detected an ARObjectAnchor will be added to the
        /// session.
        #[unsafe(method(detectionObjects))]
        #[unsafe(method_family = none)]
        pub unsafe fn detectionObjects(&self) -> Retained<NSSet<ARReferenceObject>>;

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Setter for [`detectionObjects`][Self::detectionObjects].
        #[unsafe(method(setDetectionObjects:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDetectionObjects(&self, detection_objects: &NSSet<ARReferenceObject>);

        /// Enable/disable a collaborative session. Disabled by default.
        ///
        ///
        /// When enabled, ARSession will output collaboration data for other participants using its delegate didOutputCollaborationData.
        /// It is the responsibility of the caller to send the data to each participant. When data is received by a participant, it
        /// should be passed to the ARSession by calling updateWithCollaborationData.
        #[unsafe(method(isCollaborationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isCollaborationEnabled(&self) -> bool;

        /// Setter for [`isCollaborationEnabled`][Self::isCollaborationEnabled].
        #[unsafe(method(setCollaborationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setCollaborationEnabled(&self, collaboration_enabled: bool);

        /// Indicates whether user face tracking using the front facing camera can be enabled on this device.
        #[unsafe(method(supportsUserFaceTracking))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsUserFaceTracking() -> bool;

        /// Enable or disable running Face Tracking using the front facing camera. Disabled by default.
        /// When enabled, ARSession detects faces (if visible in the front-facing camera image) and adds to its list of anchors,
        /// an ARFaceAnchor object representing each face.
        ///
        ///
        /// The transform of the ARFaceAnchor objects will be in the world coordinate space.
        ///
        /// See: ARFaceAnchor
        #[unsafe(method(userFaceTrackingEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn userFaceTrackingEnabled(&self) -> bool;

        /// Setter for [`userFaceTrackingEnabled`][Self::userFaceTrackingEnabled].
        #[unsafe(method(setUserFaceTrackingEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setUserFaceTrackingEnabled(&self, user_face_tracking_enabled: bool);

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[unsafe(method(appClipCodeTrackingEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[unsafe(method(setAppClipCodeTrackingEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[unsafe(method(supportsAppClipCodeTracking))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        /// Indicates whether the scene reconstruction type is supported for the configuration on this device.
        #[unsafe(method(supportsSceneReconstruction:))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsSceneReconstruction(
            scene_reconstruction: ARSceneReconstruction,
        ) -> bool;

        /// Type of scene reconstruction to run. Defaults to ARSceneReconstructionNone.
        ///
        /// See: ARMeshAnchor
        ///
        /// If set to a value other than ARSceneReconstructionNone, output of scene reconstruction will be added to the session as
        /// ARMeshAnchor objects.
        #[unsafe(method(sceneReconstruction))]
        #[unsafe(method_family = none)]
        pub unsafe fn sceneReconstruction(&self) -> ARSceneReconstruction;

        /// Setter for [`sceneReconstruction`][Self::sceneReconstruction].
        #[unsafe(method(setSceneReconstruction:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setSceneReconstruction(&self, scene_reconstruction: ARSceneReconstruction);

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running orientation tracking.
    ///
    ///
    /// Orientation tracking provides 3 degrees of freedom tracking of the device.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct AROrientationTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for AROrientationTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for AROrientationTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for AROrientationTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl AROrientationTrackingConfiguration {
    extern_methods!(
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[unsafe(method(isAutoFocusEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[unsafe(method(setAutoFocusEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running face tracking.
    ///
    ///
    /// Face tracking uses the front facing camera to track the face in 3D providing details on the topology and expression of the face.
    /// A detected face will be added to the session as an ARFaceAnchor object which contains information about head pose, mesh, eye pose, and blend shape
    /// coefficients. If light estimation is enabled the detected face will be treated as a light probe and used to estimate the direction of incoming light.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARFaceTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARFaceTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARFaceTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARFaceTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARFaceTrackingConfiguration {
    extern_methods!(
        /// Maximum number of faces which can be tracked simultaneously.
        #[unsafe(method(supportedNumberOfTrackedFaces))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportedNumberOfTrackedFaces() -> NSInteger;

        /// Maximum number of faces to track simultaneously.
        ///
        /// Setting the maximum number of tracked faces will limit the number of faces that can be tracked in a given frame.
        /// If more than the maximum is visible, only the faces already being tracked will continue to track until tracking is lost or another face is removed.
        /// Default value is one.
        #[unsafe(method(maximumNumberOfTrackedFaces))]
        #[unsafe(method_family = none)]
        pub unsafe fn maximumNumberOfTrackedFaces(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedFaces`][Self::maximumNumberOfTrackedFaces].
        #[unsafe(method(setMaximumNumberOfTrackedFaces:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setMaximumNumberOfTrackedFaces(
            &self,
            maximum_number_of_tracked_faces: NSInteger,
        );

        /// Indicates whether world tracking can be enabled on this device.
        #[unsafe(method(supportsWorldTracking))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsWorldTracking() -> bool;

        /// Enable or disable World Tracking. Disabled by default.
        ///
        ///
        /// When enabled, ARSession uses the back facing camera to track the device's orientation and position in the world. The camera transform and
        /// the ARFaceAnchor transform will be in the world coordinate space.
        #[unsafe(method(isWorldTrackingEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isWorldTrackingEnabled(&self) -> bool;

        /// Setter for [`isWorldTrackingEnabled`][Self::isWorldTrackingEnabled].
        #[unsafe(method(setWorldTrackingEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWorldTrackingEnabled(&self, world_tracking_enabled: bool);

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running image tracking.
    ///
    ///
    /// Image tracking provides 6 degrees of freedom tracking of known images. Four images may be tracked simultaneously.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARImageTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARImageTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARImageTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARImageTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARImageTrackingConfiguration {
    extern_methods!(
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[unsafe(method(isAutoFocusEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[unsafe(method(setAutoFocusEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to track in the scene.
        #[unsafe(method(trackingImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn trackingImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`trackingImages`][Self::trackingImages].
        #[unsafe(method(setTrackingImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setTrackingImages(&self, tracking_images: &NSSet<ARReferenceImage>);

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Default value is one.
        #[unsafe(method(maximumNumberOfTrackedImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[unsafe(method(setMaximumNumberOfTrackedImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for scanning objects.
    ///
    ///
    /// The object scanning configuration runs world tracking, capturing additional detail in order to create reference objects.
    /// Running object scanning will consume additional power in order to provide more detailed features.
    /// The createReferenceObject method can be called on the session to capture a scan of an object in the world.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARObjectScanningConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARObjectScanningConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARObjectScanningConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARObjectScanningConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARObjectScanningConfiguration {
    extern_methods!(
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[unsafe(method(isAutoFocusEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[unsafe(method(setAutoFocusEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[unsafe(method(planeDetection))]
        #[unsafe(method_family = none)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[unsafe(method(setPlaneDetection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running body tracking.
    ///
    ///
    /// Body tracking provides 6 degrees of freedom tracking of a detected body in the scene. By default, ARFrameSemanticBodyDetection will be
    /// enabled.
    ///
    /// See: ARBodyAnchor
    ///
    /// See: -[ARFrame detectedBody]
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARBodyTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARBodyTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARBodyTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARBodyTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARBodyTrackingConfiguration {
    extern_methods!(
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[unsafe(method(isAutoFocusEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[unsafe(method(setAutoFocusEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[unsafe(method(initialWorldMap))]
        #[unsafe(method_family = none)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[unsafe(method(setInitialWorldMap:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[unsafe(method(environmentTexturing))]
        #[unsafe(method_family = none)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[unsafe(method(setEnvironmentTexturing:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[unsafe(method(wantsHDREnvironmentTextures))]
        #[unsafe(method_family = none)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[unsafe(method(setWantsHDREnvironmentTextures:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[unsafe(method(planeDetection))]
        #[unsafe(method_family = none)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[unsafe(method(setPlaneDetection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[unsafe(method(detectionImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[unsafe(method(setDetectionImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDetectionImages(&self, detection_images: &NSSet<ARReferenceImage>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[unsafe(method(automaticImageScaleEstimationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[unsafe(method(setAutomaticImageScaleEstimationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Enables the estimation of a scale factor which may be used to correct the physical size of a skeleton in 3D.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the default one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARBodyAnchor.
        ///
        /// Note: When set to true the transform of a returned ARBodyAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[unsafe(method(automaticSkeletonScaleEstimationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn automaticSkeletonScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticSkeletonScaleEstimationEnabled`][Self::automaticSkeletonScaleEstimationEnabled].
        #[unsafe(method(setAutomaticSkeletonScaleEstimationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutomaticSkeletonScaleEstimationEnabled(
            &self,
            automatic_skeleton_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[unsafe(method(maximumNumberOfTrackedImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[unsafe(method(setMaximumNumberOfTrackedImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[unsafe(method(appClipCodeTrackingEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[unsafe(method(setAppClipCodeTrackingEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[unsafe(method(supportsAppClipCodeTracking))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running positional tracking.
    ///
    ///
    /// Positional tracking provides 6 degrees of freedom tracking of the device by running the camera at lowest possible resolution and frame
    /// rate.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARPositionalTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARPositionalTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARPositionalTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARPositionalTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARPositionalTrackingConfiguration {
    extern_methods!(
        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[unsafe(method(planeDetection))]
        #[unsafe(method_family = none)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[unsafe(method(setPlaneDetection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[unsafe(method(initialWorldMap))]
        #[unsafe(method_family = none)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[unsafe(method(setInitialWorldMap:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running geographical world tracking.
    ///
    ///
    /// It allows placing geo-referenced anchors (ARGeoAnchor) in the scene by running world tracking with location and compass.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/argeotrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARGeoTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
extern_conformance!(
    unsafe impl NSCopying for ARGeoTrackingConfiguration {}
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARGeoTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
extern_conformance!(
    unsafe impl NSObjectProtocol for ARGeoTrackingConfiguration {}
);

#[cfg(feature = "objc2")]
impl ARGeoTrackingConfiguration {
    extern_methods!(
        /// Unavailable
        #[unsafe(method(worldAlignment))]
        #[unsafe(method_family = none)]
        pub unsafe fn worldAlignment(&self) -> ARWorldAlignment;

        /// Setter for [`worldAlignment`][Self::worldAlignment].
        #[unsafe(method(setWorldAlignment:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWorldAlignment(&self, world_alignment: ARWorldAlignment);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[unsafe(method(environmentTexturing))]
        #[unsafe(method_family = none)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[unsafe(method(setEnvironmentTexturing:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[unsafe(method(wantsHDREnvironmentTextures))]
        #[unsafe(method_family = none)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[unsafe(method(setWantsHDREnvironmentTextures:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[unsafe(method(planeDetection))]
        #[unsafe(method_family = none)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[unsafe(method(setPlaneDetection:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[unsafe(method(detectionImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[unsafe(method(setDetectionImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDetectionImages(&self, detection_images: Option<&NSSet<ARReferenceImage>>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[unsafe(method(automaticImageScaleEstimationEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[unsafe(method(setAutomaticImageScaleEstimationEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[unsafe(method(maximumNumberOfTrackedImages))]
        #[unsafe(method_family = none)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[unsafe(method(setMaximumNumberOfTrackedImages:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Objects to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified objects. When an object is detected an ARObjectAnchor will be added to the
        /// session.
        #[unsafe(method(detectionObjects))]
        #[unsafe(method_family = none)]
        pub unsafe fn detectionObjects(&self) -> Retained<NSSet<ARReferenceObject>>;

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Setter for [`detectionObjects`][Self::detectionObjects].
        #[unsafe(method(setDetectionObjects:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setDetectionObjects(&self, detection_objects: &NSSet<ARReferenceObject>);

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[unsafe(method(appClipCodeTrackingEnabled))]
        #[unsafe(method_family = none)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[unsafe(method(setAppClipCodeTrackingEnabled:))]
        #[unsafe(method_family = none)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[unsafe(method(supportsAppClipCodeTracking))]
        #[unsafe(method_family = none)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        #[cfg(all(feature = "block2", feature = "objc2-foundation"))]
        /// Determines the availability of geo tracking at the current location.
        ///
        ///
        /// This method will attempt to acquire a location fix on a background thread, then check availability.
        ///
        ///
        /// Parameter `completionHandler`: Completion handler that is called when availability has been determined. This handler is executed on an arbitrary serial
        /// queue. It takes the following parameters: isAvailable - True if geo tracking is available at the current location, otherwise false. error - An error
        /// that indicates why geo tracking is not available at the current location.
        #[unsafe(method(checkAvailabilityWithCompletionHandler:))]
        #[unsafe(method_family = none)]
        pub unsafe fn checkAvailabilityWithCompletionHandler(
            completion_handler: &block2::DynBlock<dyn Fn(Bool, *mut NSError)>,
        );

        #[cfg(all(
            feature = "block2",
            feature = "objc2-core-location",
            feature = "objc2-foundation"
        ))]
        /// Determines the availability of geo tracking at the given location.
        ///
        /// Parameter `coordinate`: Location at which to check.
        ///
        /// Parameter `completionHandler`: Completion handler that is called when availability has been determined. This handler is executed on an arbitrary serial
        /// queue. It takes the following parameters: isAvailable - True if geo tracking is available at the given location, otherwise false. error - An error
        /// that indicates why geo tracking is not available at the given location.
        #[unsafe(method(checkAvailabilityAtCoordinate:completionHandler:))]
        #[unsafe(method_family = none)]
        pub unsafe fn checkAvailabilityAtCoordinate_completionHandler(
            coordinate: CLLocationCoordinate2D,
            completion_handler: &block2::DynBlock<dyn Fn(Bool, *mut NSError)>,
        );

        #[unsafe(method(init))]
        #[unsafe(method_family = init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[unsafe(method(new))]
        #[unsafe(method_family = new)]
        pub unsafe fn new() -> Retained<Self>;
    );
}
